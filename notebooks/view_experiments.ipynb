{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification Baselines on MATH Dataset - Results\n",
    "\n",
    "This notebook displays the results from fine-tuning experiments on the classification task using the MATH dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yacinebenihaddadene/Documents/School/S9/NLP1/Project/FewShotOptimisation/AdvancedNLP-Project/experiment_results/classification/0_supervised_baselines_results\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "RESULTS_PATH = \"../experiment_results/classification/0_supervised_baselines_results\"\n",
    "\n",
    "try:\n",
    "    os.chdir(RESULTS_PATH)\n",
    "    print(os.getcwd())\n",
    "except Exception:\n",
    "    raise ValueError(f\"Experiment results path not found: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth Generative Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments: 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model_path</th>\n",
       "      <th>model_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>lora_rank</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>Algebra_precision</th>\n",
       "      <th>Algebra_recall</th>\n",
       "      <th>Algebra_f1</th>\n",
       "      <th>Algebra_support</th>\n",
       "      <th>Counting &amp; Probability_precision</th>\n",
       "      <th>Counting &amp; Probability_recall</th>\n",
       "      <th>Counting &amp; Probability_f1</th>\n",
       "      <th>Counting &amp; Probability_support</th>\n",
       "      <th>Geometry_precision</th>\n",
       "      <th>Geometry_recall</th>\n",
       "      <th>Geometry_f1</th>\n",
       "      <th>Geometry_support</th>\n",
       "      <th>Intermediate Algebra_precision</th>\n",
       "      <th>Intermediate Algebra_recall</th>\n",
       "      <th>Intermediate Algebra_f1</th>\n",
       "      <th>Intermediate Algebra_support</th>\n",
       "      <th>Number Theory_precision</th>\n",
       "      <th>Number Theory_recall</th>\n",
       "      <th>Number Theory_f1</th>\n",
       "      <th>Number Theory_support</th>\n",
       "      <th>Prealgebra_precision</th>\n",
       "      <th>Prealgebra_recall</th>\n",
       "      <th>Prealgebra_f1</th>\n",
       "      <th>Prealgebra_support</th>\n",
       "      <th>Precalculus_precision</th>\n",
       "      <th>Precalculus_recall</th>\n",
       "      <th>Precalculus_f1</th>\n",
       "      <th>Precalculus_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-13 18:22:25</td>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>21.270479</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.762611</td>\n",
       "      <td>0.924460</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.593985</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.955224</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.756906</td>\n",
       "      <td>0.925676</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.481651</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-13 19:23:12</td>\n",
       "      <td>fine_tunings/unsloth/finetune_gemma_7b</td>\n",
       "      <td>gemma:7b</td>\n",
       "      <td>finetune_gemma_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.265760</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-13 19:33:17</td>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>21.294834</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.762611</td>\n",
       "      <td>0.924460</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.593985</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.955224</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.756906</td>\n",
       "      <td>0.925676</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.481651</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-13 19:43:23</td>\n",
       "      <td>fine_tunings/unsloth/finetune_mistral_7b</td>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.842657</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.822848</td>\n",
       "      <td>21.344999</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.787770</td>\n",
       "      <td>0.845560</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.931330</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.620301</td>\n",
       "      <td>0.756881</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.942149</td>\n",
       "      <td>0.953975</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-13 19:51:18</td>\n",
       "      <td>fine_tunings/unsloth/finetune_phi3_mini</td>\n",
       "      <td>phi3:mini</td>\n",
       "      <td>finetune_phi3_mini</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.735949</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.669414</td>\n",
       "      <td>27.218389</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.852761</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.630385</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.660550</td>\n",
       "      <td>0.541353</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.651652</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.641892</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.536697</td>\n",
       "      <td>0.554502</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.747573</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-01-13 19:57:23</td>\n",
       "      <td>fine_tunings/unsloth/finetune_qwen2_7b</td>\n",
       "      <td>qwen2:7b</td>\n",
       "      <td>finetune_qwen2_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.795759</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.759331</td>\n",
       "      <td>15.286895</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.616541</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.601399</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-01-13 20:12:39</td>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.833848</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-01-13 20:21:49</td>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.479597</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>21.966956</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.349693</td>\n",
       "      <td>0.410072</td>\n",
       "      <td>0.377483</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.630137</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.440559</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.293375</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.337568</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.096330</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026-01-13 20:31:00</td>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.114706</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>22.411656</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.100917</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026-01-13 20:37:25</td>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.977378</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2026-01-13 20:43:17</td>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.436731</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.407763</td>\n",
       "      <td>13.626541</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.474820</td>\n",
       "      <td>0.468917</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.602317</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.469880</td>\n",
       "      <td>0.879699</td>\n",
       "      <td>0.612565</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.410377</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.390135</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.524887</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.628726</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.194757</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              timestamp                                model_path  \\\n",
       "0   2026-01-13 18:22:25   fine_tunings/unsloth/finetune_llama3_8b   \n",
       "1   2026-01-13 19:23:12    fine_tunings/unsloth/finetune_gemma_7b   \n",
       "2   2026-01-13 19:33:17   fine_tunings/unsloth/finetune_llama3_8b   \n",
       "3   2026-01-13 19:43:23  fine_tunings/unsloth/finetune_mistral_7b   \n",
       "4   2026-01-13 19:51:18   fine_tunings/unsloth/finetune_phi3_mini   \n",
       "5   2026-01-13 19:57:23    fine_tunings/unsloth/finetune_qwen2_7b   \n",
       "6   2026-01-13 20:12:39                 unsloth/gemma-7b-bnb-4bit   \n",
       "7   2026-01-13 20:21:49               unsloth/llama-3-8b-bnb-4bit   \n",
       "8   2026-01-13 20:31:00          unsloth/mistral-7b-v0.3-bnb-4bit   \n",
       "9   2026-01-13 20:37:25   unsloth/Phi-3-mini-4k-instruct-bnb-4bit   \n",
       "10  2026-01-13 20:43:17                 unsloth/Qwen2-7B-bnb-4bit   \n",
       "\n",
       "                                 model_name             run_name  \\\n",
       "0                                 llama3:8b   finetune_llama3_8b   \n",
       "1                                  gemma:7b    finetune_gemma_7b   \n",
       "2                                 llama3:8b   finetune_llama3_8b   \n",
       "3                                mistral:7b  finetune_mistral_7b   \n",
       "4                                 phi3:mini   finetune_phi3_mini   \n",
       "5                                  qwen2:7b    finetune_qwen2_7b   \n",
       "6                 unsloth/gemma-7b-bnb-4bit      base_model_eval   \n",
       "7               unsloth/llama-3-8b-bnb-4bit      base_model_eval   \n",
       "8          unsloth/mistral-7b-v0.3-bnb-4bit      base_model_eval   \n",
       "9   unsloth/Phi-3-mini-4k-instruct-bnb-4bit      base_model_eval   \n",
       "10                unsloth/Qwen2-7B-bnb-4bit      base_model_eval   \n",
       "\n",
       "                      dataset  test_size  accuracy  precision_weighted  \\\n",
       "0   qwedsacf/competition_math       1250    0.7760            0.827533   \n",
       "1   qwedsacf/competition_math       1250    0.0000            0.000000   \n",
       "2   qwedsacf/competition_math       1250    0.7760            0.827533   \n",
       "3   qwedsacf/competition_math       1250    0.8136            0.842657   \n",
       "4   qwedsacf/competition_math       1250    0.6408            0.735949   \n",
       "5   qwedsacf/competition_math       1250    0.7528            0.795759   \n",
       "6   qwedsacf/competition_math       1250    0.0000            0.000000   \n",
       "7   qwedsacf/competition_math       1250    0.3624            0.479597   \n",
       "8   qwedsacf/competition_math       1250    0.0336            0.114706   \n",
       "9   qwedsacf/competition_math       1250    0.0000            0.000000   \n",
       "10  qwedsacf/competition_math       1250    0.4448            0.436731   \n",
       "\n",
       "    recall_weighted  f1_weighted  tokens_per_sec  num_epochs  batch_size  \\\n",
       "0            0.7760     0.789775       21.270479           3           2   \n",
       "1            0.0000     0.000000       19.265760           3           2   \n",
       "2            0.7760     0.789775       21.294834           3           2   \n",
       "3            0.8136     0.822848       21.344999           3           2   \n",
       "4            0.6408     0.669414       27.218389           3           2   \n",
       "5            0.7528     0.759331       15.286895           3           2   \n",
       "6            0.0000     0.000000       18.833848           0           2   \n",
       "7            0.3624     0.334838       21.966956           0           2   \n",
       "8            0.0336     0.048908       22.411656           0           2   \n",
       "9            0.0000     0.000000       37.977378           0           2   \n",
       "10           0.4448     0.407763       13.626541           0           2   \n",
       "\n",
       "    learning_rate  lora_rank  lora_alpha  Algebra_precision  Algebra_recall  \\\n",
       "0          0.0002         16          16           0.762611        0.924460   \n",
       "1          0.0002         16          16           0.000000        0.000000   \n",
       "2          0.0002         16          16           0.762611        0.924460   \n",
       "3          0.0002         16          16           0.912500        0.787770   \n",
       "4          0.0002         16          16           0.852761        0.500000   \n",
       "5          0.0002         16          16           0.926966        0.593525   \n",
       "6          0.0002         16          16           0.000000        0.000000   \n",
       "7          0.0002         16          16           0.349693        0.410072   \n",
       "8          0.0002         16          16           0.000000        0.000000   \n",
       "9          0.0002         16          16           0.000000        0.000000   \n",
       "10         0.0002         16          16           0.463158        0.474820   \n",
       "\n",
       "    Algebra_f1  Algebra_support  Counting & Probability_precision  \\\n",
       "0     0.835772            278.0                          0.785714   \n",
       "1     0.000000            278.0                          0.000000   \n",
       "2     0.835772            278.0                          0.785714   \n",
       "3     0.845560            278.0                          0.805085   \n",
       "4     0.630385            278.0                          0.743363   \n",
       "5     0.723684            278.0                          0.860465   \n",
       "6     0.000000            278.0                          0.000000   \n",
       "7     0.377483            278.0                          0.630137   \n",
       "8     0.000000            278.0                          0.000000   \n",
       "9     0.000000            278.0                          0.000000   \n",
       "10    0.468917            278.0                          0.553191   \n",
       "\n",
       "    Counting & Probability_recall  Counting & Probability_f1  \\\n",
       "0                        0.745763                   0.765217   \n",
       "1                        0.000000                   0.000000   \n",
       "2                        0.745763                   0.765217   \n",
       "3                        0.805085                   0.805085   \n",
       "4                        0.711864                   0.727273   \n",
       "5                        0.627119                   0.725490   \n",
       "6                        0.000000                   0.000000   \n",
       "7                        0.389831                   0.481675   \n",
       "8                        0.000000                   0.000000   \n",
       "9                        0.000000                   0.000000   \n",
       "10                       0.661017                   0.602317   \n",
       "\n",
       "    Counting & Probability_support  Geometry_precision  Geometry_recall  \\\n",
       "0                            118.0            0.822917         0.593985   \n",
       "1                            118.0            0.000000         0.000000   \n",
       "2                            118.0            0.822917         0.593985   \n",
       "3                            118.0            0.912088         0.624060   \n",
       "4                            118.0            0.660550         0.541353   \n",
       "5                            118.0            0.796117         0.616541   \n",
       "6                            118.0            0.000000         0.000000   \n",
       "7                            118.0            0.411765         0.473684   \n",
       "8                            118.0            0.250000         0.015038   \n",
       "9                            118.0            0.000000         0.000000   \n",
       "10                           118.0            0.469880         0.879699   \n",
       "\n",
       "    Geometry_f1  Geometry_support  Intermediate Algebra_precision  \\\n",
       "0      0.689956             133.0                        0.955224   \n",
       "1      0.000000             133.0                        0.000000   \n",
       "2      0.689956             133.0                        0.955224   \n",
       "3      0.741071             133.0                        0.931330   \n",
       "4      0.595041             133.0                        0.651652   \n",
       "5      0.694915             133.0                        0.769784   \n",
       "6      0.000000             133.0                        0.000000   \n",
       "7      0.440559             133.0                        0.293375   \n",
       "8      0.028369             133.0                        0.000000   \n",
       "9      0.000000             133.0                        0.000000   \n",
       "10     0.612565             133.0                        0.410377   \n",
       "\n",
       "    Intermediate Algebra_recall  Intermediate Algebra_f1  \\\n",
       "0                      0.820513                 0.882759   \n",
       "1                      0.000000                 0.000000   \n",
       "2                      0.820513                 0.882759   \n",
       "3                      0.927350                 0.929336   \n",
       "4                      0.927350                 0.765432   \n",
       "5                      0.914530                 0.835938   \n",
       "6                      0.000000                 0.000000   \n",
       "7                      0.397436                 0.337568   \n",
       "8                      0.000000                 0.000000   \n",
       "9                      0.000000                 0.000000   \n",
       "10                     0.371795                 0.390135   \n",
       "\n",
       "    Intermediate Algebra_support  Number Theory_precision  \\\n",
       "0                          234.0                 0.756906   \n",
       "1                          234.0                 0.000000   \n",
       "2                          234.0                 0.756906   \n",
       "3                          234.0                 0.765432   \n",
       "4                          234.0                 0.811966   \n",
       "5                          234.0                 0.802721   \n",
       "6                          234.0                 0.000000   \n",
       "7                          234.0                 0.425926   \n",
       "8                          234.0                 0.000000   \n",
       "9                          234.0                 0.000000   \n",
       "10                         234.0                 0.524887   \n",
       "\n",
       "    Number Theory_recall  Number Theory_f1  Number Theory_support  \\\n",
       "0               0.925676          0.832827                  148.0   \n",
       "1               0.000000          0.000000                  148.0   \n",
       "2               0.925676          0.832827                  148.0   \n",
       "3               0.837838          0.800000                  148.0   \n",
       "4               0.641892          0.716981                  148.0   \n",
       "5               0.797297          0.800000                  148.0   \n",
       "6               0.000000          0.000000                  148.0   \n",
       "7               0.777027          0.550239                  148.0   \n",
       "8               0.000000          0.000000                  148.0   \n",
       "9               0.000000          0.000000                  148.0   \n",
       "10              0.783784          0.628726                  148.0   \n",
       "\n",
       "    Prealgebra_precision  Prealgebra_recall  Prealgebra_f1  \\\n",
       "0               0.783582           0.481651       0.596591   \n",
       "1               0.000000           0.000000       0.000000   \n",
       "2               0.783582           0.481651       0.596591   \n",
       "3               0.620301           0.756881       0.681818   \n",
       "4               0.573529           0.536697       0.554502   \n",
       "5               0.601399           0.788991       0.682540   \n",
       "6               0.000000           0.000000       0.000000   \n",
       "7               0.552632           0.096330       0.164062   \n",
       "8               0.192982           0.100917       0.132530   \n",
       "9               0.000000           0.000000       0.000000   \n",
       "10              0.530612           0.119266       0.194757   \n",
       "\n",
       "    Prealgebra_support  Precalculus_precision  Precalculus_recall  \\\n",
       "0                218.0               0.941176            0.925620   \n",
       "1                218.0               0.000000            0.000000   \n",
       "2                218.0               0.941176            0.925620   \n",
       "3                218.0               0.966102            0.942149   \n",
       "4                218.0               0.905882            0.636364   \n",
       "5                218.0               0.822695            0.958678   \n",
       "6                218.0               0.000000            0.000000   \n",
       "7                218.0               1.000000            0.008264   \n",
       "8                218.0               0.562500            0.148760   \n",
       "9                218.0               0.000000            0.000000   \n",
       "10               218.0               0.000000            0.000000   \n",
       "\n",
       "    Precalculus_f1  Precalculus_support  \n",
       "0         0.933333                121.0  \n",
       "1         0.000000                121.0  \n",
       "2         0.933333                121.0  \n",
       "3         0.953975                121.0  \n",
       "4         0.747573                121.0  \n",
       "5         0.885496                121.0  \n",
       "6         0.000000                121.0  \n",
       "7         0.016393                121.0  \n",
       "8         0.235294                121.0  \n",
       "9         0.000000                121.0  \n",
       "10        0.000000                121.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('experiments.csv')\n",
    "\n",
    "print(f\"Total experiments: {len(df)}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine_tunings/unsloth/finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.822848</td>\n",
       "      <td>0.842657</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>21.344999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>21.270479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>21.294834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fine_tunings/unsloth/finetune_qwen2_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.759331</td>\n",
       "      <td>0.795759</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>15.286895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fine_tunings/unsloth/finetune_phi3_mini</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.669414</td>\n",
       "      <td>0.735949</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>27.218389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.407763</td>\n",
       "      <td>0.436731</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>13.626541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>0.479597</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>21.966956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>0.114706</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>22.411656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fine_tunings/unsloth/finetune_gemma_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>19.265760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>18.833848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>37.977378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model_path                    dataset  \\\n",
       "3   fine_tunings/unsloth/finetune_mistral_7b  qwedsacf/competition_math   \n",
       "0    fine_tunings/unsloth/finetune_llama3_8b  qwedsacf/competition_math   \n",
       "2    fine_tunings/unsloth/finetune_llama3_8b  qwedsacf/competition_math   \n",
       "5     fine_tunings/unsloth/finetune_qwen2_7b  qwedsacf/competition_math   \n",
       "4    fine_tunings/unsloth/finetune_phi3_mini  qwedsacf/competition_math   \n",
       "10                 unsloth/Qwen2-7B-bnb-4bit  qwedsacf/competition_math   \n",
       "7                unsloth/llama-3-8b-bnb-4bit  qwedsacf/competition_math   \n",
       "8           unsloth/mistral-7b-v0.3-bnb-4bit  qwedsacf/competition_math   \n",
       "1     fine_tunings/unsloth/finetune_gemma_7b  qwedsacf/competition_math   \n",
       "6                  unsloth/gemma-7b-bnb-4bit  qwedsacf/competition_math   \n",
       "9    unsloth/Phi-3-mini-4k-instruct-bnb-4bit  qwedsacf/competition_math   \n",
       "\n",
       "    test_size  accuracy  f1_weighted  precision_weighted  recall_weighted  \\\n",
       "3        1250    0.8136     0.822848            0.842657           0.8136   \n",
       "0        1250    0.7760     0.789775            0.827533           0.7760   \n",
       "2        1250    0.7760     0.789775            0.827533           0.7760   \n",
       "5        1250    0.7528     0.759331            0.795759           0.7528   \n",
       "4        1250    0.6408     0.669414            0.735949           0.6408   \n",
       "10       1250    0.4448     0.407763            0.436731           0.4448   \n",
       "7        1250    0.3624     0.334838            0.479597           0.3624   \n",
       "8        1250    0.0336     0.048908            0.114706           0.0336   \n",
       "1        1250    0.0000     0.000000            0.000000           0.0000   \n",
       "6        1250    0.0000     0.000000            0.000000           0.0000   \n",
       "9        1250    0.0000     0.000000            0.000000           0.0000   \n",
       "\n",
       "    tokens_per_sec  \n",
       "3        21.344999  \n",
       "0        21.270479  \n",
       "2        21.294834  \n",
       "5        15.286895  \n",
       "4        27.218389  \n",
       "10       13.626541  \n",
       "7        21.966956  \n",
       "8        22.411656  \n",
       "1        19.265760  \n",
       "6        18.833848  \n",
       "9        37.977378  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key metrics comparison\n",
    "summary_cols = ['model_path', 'dataset', 'test_size', 'accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted', 'tokens_per_sec']\n",
    "available_cols = [c for c in summary_cols if c in df.columns]\n",
    "df[available_cols].sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 per-class F1 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>Algebra</th>\n",
       "      <th>Counting &amp; Probability</th>\n",
       "      <th>Geometry</th>\n",
       "      <th>Intermediate Algebra</th>\n",
       "      <th>Number Theory</th>\n",
       "      <th>Prealgebra</th>\n",
       "      <th>Precalculus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fine_tunings/unsloth/finetune_gemma_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine_tunings/unsloth/finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.845560</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.953975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fine_tunings/unsloth/finetune_phi3_mini</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.630385</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.554502</td>\n",
       "      <td>0.747573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fine_tunings/unsloth/finetune_qwen2_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.885496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.377483</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>0.440559</td>\n",
       "      <td>0.337568</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.016393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.468917</td>\n",
       "      <td>0.602317</td>\n",
       "      <td>0.612565</td>\n",
       "      <td>0.390135</td>\n",
       "      <td>0.628726</td>\n",
       "      <td>0.194757</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model_path                    dataset  \\\n",
       "0    fine_tunings/unsloth/finetune_llama3_8b  qwedsacf/competition_math   \n",
       "1     fine_tunings/unsloth/finetune_gemma_7b  qwedsacf/competition_math   \n",
       "2    fine_tunings/unsloth/finetune_llama3_8b  qwedsacf/competition_math   \n",
       "3   fine_tunings/unsloth/finetune_mistral_7b  qwedsacf/competition_math   \n",
       "4    fine_tunings/unsloth/finetune_phi3_mini  qwedsacf/competition_math   \n",
       "5     fine_tunings/unsloth/finetune_qwen2_7b  qwedsacf/competition_math   \n",
       "6                  unsloth/gemma-7b-bnb-4bit  qwedsacf/competition_math   \n",
       "7                unsloth/llama-3-8b-bnb-4bit  qwedsacf/competition_math   \n",
       "8           unsloth/mistral-7b-v0.3-bnb-4bit  qwedsacf/competition_math   \n",
       "9    unsloth/Phi-3-mini-4k-instruct-bnb-4bit  qwedsacf/competition_math   \n",
       "10                 unsloth/Qwen2-7B-bnb-4bit  qwedsacf/competition_math   \n",
       "\n",
       "    test_size   Algebra  Counting & Probability  Geometry  \\\n",
       "0        1250  0.835772                0.765217  0.689956   \n",
       "1        1250  0.000000                0.000000  0.000000   \n",
       "2        1250  0.835772                0.765217  0.689956   \n",
       "3        1250  0.845560                0.805085  0.741071   \n",
       "4        1250  0.630385                0.727273  0.595041   \n",
       "5        1250  0.723684                0.725490  0.694915   \n",
       "6        1250  0.000000                0.000000  0.000000   \n",
       "7        1250  0.377483                0.481675  0.440559   \n",
       "8        1250  0.000000                0.000000  0.028369   \n",
       "9        1250  0.000000                0.000000  0.000000   \n",
       "10       1250  0.468917                0.602317  0.612565   \n",
       "\n",
       "    Intermediate Algebra  Number Theory  Prealgebra  Precalculus  \n",
       "0               0.882759       0.832827    0.596591     0.933333  \n",
       "1               0.000000       0.000000    0.000000     0.000000  \n",
       "2               0.882759       0.832827    0.596591     0.933333  \n",
       "3               0.929336       0.800000    0.681818     0.953975  \n",
       "4               0.765432       0.716981    0.554502     0.747573  \n",
       "5               0.835938       0.800000    0.682540     0.885496  \n",
       "6               0.000000       0.000000    0.000000     0.000000  \n",
       "7               0.337568       0.550239    0.164062     0.016393  \n",
       "8               0.000000       0.000000    0.132530     0.235294  \n",
       "9               0.000000       0.000000    0.000000     0.000000  \n",
       "10              0.390135       0.628726    0.194757     0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract per-class F1 columns\n",
    "f1_cols = [c for c in df.columns if c.endswith('_f1') and c != 'f1_weighted']\n",
    "print(f\"Found {len(f1_cols)} per-class F1 columns.\")\n",
    "if f1_cols:\n",
    "    display_df = df[['model_path', 'dataset', 'test_size',] + f1_cols].copy()\n",
    "    display_df.columns = ['model_path', 'dataset', 'test_size',] + [c.replace('_f1', '') for c in f1_cols]\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>model_name</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>lora_rank</th>\n",
       "      <th>lora_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fine_tunings/unsloth/finetune_gemma_7b</td>\n",
       "      <td>gemma:7b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine_tunings/unsloth/finetune_mistral_7b</td>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fine_tunings/unsloth/finetune_phi3_mini</td>\n",
       "      <td>phi3:mini</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fine_tunings/unsloth/finetune_qwen2_7b</td>\n",
       "      <td>qwen2:7b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model_path  \\\n",
       "0    fine_tunings/unsloth/finetune_llama3_8b   \n",
       "1     fine_tunings/unsloth/finetune_gemma_7b   \n",
       "2    fine_tunings/unsloth/finetune_llama3_8b   \n",
       "3   fine_tunings/unsloth/finetune_mistral_7b   \n",
       "4    fine_tunings/unsloth/finetune_phi3_mini   \n",
       "5     fine_tunings/unsloth/finetune_qwen2_7b   \n",
       "6                  unsloth/gemma-7b-bnb-4bit   \n",
       "7                unsloth/llama-3-8b-bnb-4bit   \n",
       "8           unsloth/mistral-7b-v0.3-bnb-4bit   \n",
       "9    unsloth/Phi-3-mini-4k-instruct-bnb-4bit   \n",
       "10                 unsloth/Qwen2-7B-bnb-4bit   \n",
       "\n",
       "                                 model_name  num_epochs  batch_size  \\\n",
       "0                                 llama3:8b           3           2   \n",
       "1                                  gemma:7b           3           2   \n",
       "2                                 llama3:8b           3           2   \n",
       "3                                mistral:7b           3           2   \n",
       "4                                 phi3:mini           3           2   \n",
       "5                                  qwen2:7b           3           2   \n",
       "6                 unsloth/gemma-7b-bnb-4bit           0           2   \n",
       "7               unsloth/llama-3-8b-bnb-4bit           0           2   \n",
       "8          unsloth/mistral-7b-v0.3-bnb-4bit           0           2   \n",
       "9   unsloth/Phi-3-mini-4k-instruct-bnb-4bit           0           2   \n",
       "10                unsloth/Qwen2-7B-bnb-4bit           0           2   \n",
       "\n",
       "    learning_rate  lora_rank  lora_alpha  \n",
       "0          0.0002         16          16  \n",
       "1          0.0002         16          16  \n",
       "2          0.0002         16          16  \n",
       "3          0.0002         16          16  \n",
       "4          0.0002         16          16  \n",
       "5          0.0002         16          16  \n",
       "6          0.0002         16          16  \n",
       "7          0.0002         16          16  \n",
       "8          0.0002         16          16  \n",
       "9          0.0002         16          16  \n",
       "10         0.0002         16          16  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config columns\n",
    "config_cols = ['model_path', 'model_name', 'num_epochs', 'batch_size', 'learning_rate', 'lora_rank', 'lora_alpha']\n",
    "available_config = [c for c in config_cols if c in df.columns]\n",
    "df[available_config]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Experiments: Generative + Discriminative + Zero-shot base models Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Finetune</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Weighted</th>\n",
       "      <th>Avg_Latency_Seconds</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qwen2:7b</td>\n",
       "      <td>classification head classification finetune</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.849699</td>\n",
       "      <td>0.144110</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>classification head classification finetune</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.832554</td>\n",
       "      <td>0.156486</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.822848</td>\n",
       "      <td>0.468494</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>classification head classification finetune</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.805375</td>\n",
       "      <td>0.177346</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>0.469597</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qwen2:7b</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.759331</td>\n",
       "      <td>0.292904</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phi3:mini</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.669414</td>\n",
       "      <td>0.367399</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.407763</td>\n",
       "      <td>0.210061</td>\n",
       "      <td>Zero-shot generative classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>0.426568</td>\n",
       "      <td>Zero-shot generative classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>0.420139</td>\n",
       "      <td>Zero-shot generative classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma:7b</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.518889</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447280</td>\n",
       "      <td>Zero-shot generative classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263315</td>\n",
       "      <td>Zero-shot generative classification</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Model  \\\n",
       "1                                  qwen2:7b   \n",
       "2                                 llama3:8b   \n",
       "5                                mistral:7b   \n",
       "0                                mistral:7b   \n",
       "4                                 llama3:8b   \n",
       "7                                  qwen2:7b   \n",
       "6                                 phi3:mini   \n",
       "12                unsloth/Qwen2-7B-bnb-4bit   \n",
       "9               unsloth/llama-3-8b-bnb-4bit   \n",
       "10         unsloth/mistral-7b-v0.3-bnb-4bit   \n",
       "3                                  gemma:7b   \n",
       "8                 unsloth/gemma-7b-bnb-4bit   \n",
       "11  unsloth/Phi-3-mini-4k-instruct-bnb-4bit   \n",
       "\n",
       "                                       Finetune  Accuracy  F1_Weighted  \\\n",
       "1   classification head classification finetune    0.8512     0.849699   \n",
       "2   classification head classification finetune    0.8328     0.832554   \n",
       "5            generative classification finetune    0.8136     0.822848   \n",
       "0   classification head classification finetune    0.8080     0.805375   \n",
       "4            generative classification finetune    0.7760     0.789775   \n",
       "7            generative classification finetune    0.7528     0.759331   \n",
       "6            generative classification finetune    0.6408     0.669414   \n",
       "12                                         None    0.4448     0.407763   \n",
       "9                                          None    0.3624     0.334838   \n",
       "10                                         None    0.0336     0.048908   \n",
       "3            generative classification finetune    0.0000     0.000000   \n",
       "8                                          None    0.0000     0.000000   \n",
       "11                                         None    0.0000     0.000000   \n",
       "\n",
       "    Avg_Latency_Seconds                                 Task  \n",
       "1              0.144110             FineTuned classification  \n",
       "2              0.156486             FineTuned classification  \n",
       "5              0.468494             FineTuned classification  \n",
       "0              0.177346             FineTuned classification  \n",
       "4              0.469597             FineTuned classification  \n",
       "7              0.292904             FineTuned classification  \n",
       "6              0.367399             FineTuned classification  \n",
       "12             0.210061  Zero-shot generative classification  \n",
       "9              0.426568  Zero-shot generative classification  \n",
       "10             0.420139  Zero-shot generative classification  \n",
       "3              0.518889             FineTuned classification  \n",
       "8              0.447280  Zero-shot generative classification  \n",
       "11             0.263315  Zero-shot generative classification  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Performance by Fine-tune Type:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Weighted</th>\n",
       "      <th>Avg_Latency_Seconds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finetune</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>0.168160</td>\n",
       "      <td>0.158302</td>\n",
       "      <td>0.353472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classification head classification finetune</th>\n",
       "      <td>0.830667</td>\n",
       "      <td>0.829209</td>\n",
       "      <td>0.159314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generative classification finetune</th>\n",
       "      <td>0.596640</td>\n",
       "      <td>0.608274</td>\n",
       "      <td>0.423457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Accuracy  F1_Weighted  \\\n",
       "Finetune                                                             \n",
       "None                                         0.168160     0.158302   \n",
       "classification head classification finetune  0.830667     0.829209   \n",
       "generative classification finetune           0.596640     0.608274   \n",
       "\n",
       "                                             Avg_Latency_Seconds  \n",
       "Finetune                                                          \n",
       "None                                                    0.353472  \n",
       "classification head classification finetune             0.159314  \n",
       "generative classification finetune                      0.423457  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Load evaluation results\n",
    "yaml_path = 'evaluation_results.yaml'\n",
    "\n",
    "try:\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        eval_data = yaml.safe_load(f)\n",
    "\n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "\n",
    "    # improved display\n",
    "    display_cols = ['Model', 'Finetune', 'Accuracy', 'F1_Weighted', 'Avg_Latency_Seconds', 'Task']\n",
    "    print(\"Evaluation Results:\")\n",
    "    display(eval_df[display_cols].sort_values('Accuracy', ascending=False))\n",
    "\n",
    "    # Grouped analysis\n",
    "    print(\"\\nAverage Performance by Fine-tune Type:\")\n",
    "    display(eval_df.groupby('Finetune')[['Accuracy', 'F1_Weighted', 'Avg_Latency_Seconds']].mean(numeric_only=True))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Could not find file at {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Result\n",
    "\n",
    "Based on the evaluation results:\n",
    "\n",
    "1.  **Discriminative Fine-tuning (Classification Head)**:\n",
    "    *   This approach consistently yields high performance.\n",
    "    *   **Qwen2:7b** is the top performer with **~85.1% accuracy**.\n",
    "    *   **Llama3:8b** (~83.3%) and **Mistral:7b** (~80.8%) also perform very well.\n",
    "    *   Latency is consistently low (~0.15s), making it efficient for pure classification tasks.\n",
    "\n",
    "2.  **Generative Fine-tuning**:\n",
    "    *   Performance is more volatile compared to the classification head approach.\n",
    "    *   **Mistral:7b** adapts best to the generative format for this task, achieving **81.4% accuracy**, which is slightly better than its discriminative counterpart.\n",
    "    *   **Llama3** and **Qwen2** see a drop in performance (77.6% and 75.3% respectively) compared to their classification head versions.\n",
    "    *   Several models (Gemma, and unsloth quantized versions of Phi-3/Mistral) struggled significantly, showing near 0% accuracy. This suggests sensitivity to hyperparameters, quantization, or output formatting in the generative setting.\n",
    "\n",
    "3.  **Overall Conclusion**:\n",
    "    *   For this specific MATH dataset classification task, **Discriminative Fine-tuning** (Class Head) appears to be the more robust and higher-performing strategy for most models, offering better accuracy and lower latency.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
