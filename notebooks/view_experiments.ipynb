{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results Viewer\n",
    "\n",
    "This notebook displays the results from fine-tuning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "run_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "test_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision_weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall_weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1_weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tokens_per_sec",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "num_epochs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "batch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lora_rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lora_alpha",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Algebra_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Algebra_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Algebra_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Algebra_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Counting & Probability_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Counting & Probability_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Counting & Probability_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Counting & Probability_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Geometry_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Geometry_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Geometry_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Geometry_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Intermediate Algebra_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Intermediate Algebra_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Intermediate Algebra_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Intermediate Algebra_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number Theory_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number Theory_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number Theory_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number Theory_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Prealgebra_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Prealgebra_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Prealgebra_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Prealgebra_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precalculus_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precalculus_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precalculus_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precalculus_support",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "d993dcc4-74c9-4efb-bb1f-2500552086cf",
       "rows": [
        [
         "0",
         "2026-01-13 18:22:25",
         "fine_tunings/unsloth/finetune_llama3_8b",
         "llama3:8b",
         "finetune_llama3_8b",
         "qwedsacf/competition_math",
         "1250",
         "0.776",
         "0.8275326984558665",
         "0.776",
         "0.7897748697439604",
         "21.270479256307315",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.7626112759643917",
         "0.9244604316546764",
         "0.8357723577235773",
         "278.0",
         "0.7857142857142857",
         "0.7457627118644068",
         "0.7652173913043478",
         "118.0",
         "0.8229166666666666",
         "0.5939849624060151",
         "0.6899563318777293",
         "133.0",
         "0.9552238805970148",
         "0.8205128205128205",
         "0.8827586206896552",
         "234.0",
         "0.7569060773480663",
         "0.9256756756756755",
         "0.8328267477203647",
         "148.0",
         "0.7835820895522388",
         "0.481651376146789",
         "0.5965909090909091",
         "218.0",
         "0.9411764705882352",
         "0.9256198347107438",
         "0.9333333333333332",
         "121.0"
        ],
        [
         "1",
         "2026-01-13 19:23:12",
         "fine_tunings/unsloth/finetune_gemma_7b",
         "gemma:7b",
         "finetune_gemma_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "19.265760125510777",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.0",
         "0.0",
         "0.0",
         "278.0",
         "0.0",
         "0.0",
         "0.0",
         "118.0",
         "0.0",
         "0.0",
         "0.0",
         "133.0",
         "0.0",
         "0.0",
         "0.0",
         "234.0",
         "0.0",
         "0.0",
         "0.0",
         "148.0",
         "0.0",
         "0.0",
         "0.0",
         "218.0",
         "0.0",
         "0.0",
         "0.0",
         "121.0"
        ],
        [
         "2",
         "2026-01-13 19:33:17",
         "fine_tunings/unsloth/finetune_llama3_8b",
         "llama3:8b",
         "finetune_llama3_8b",
         "qwedsacf/competition_math",
         "1250",
         "0.776",
         "0.8275326984558665",
         "0.776",
         "0.7897748697439604",
         "21.29483373828777",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.7626112759643917",
         "0.9244604316546764",
         "0.8357723577235773",
         "278.0",
         "0.7857142857142857",
         "0.7457627118644068",
         "0.7652173913043478",
         "118.0",
         "0.8229166666666666",
         "0.5939849624060151",
         "0.6899563318777293",
         "133.0",
         "0.9552238805970148",
         "0.8205128205128205",
         "0.8827586206896552",
         "234.0",
         "0.7569060773480663",
         "0.9256756756756755",
         "0.8328267477203647",
         "148.0",
         "0.7835820895522388",
         "0.481651376146789",
         "0.5965909090909091",
         "218.0",
         "0.9411764705882352",
         "0.9256198347107438",
         "0.9333333333333332",
         "121.0"
        ],
        [
         "3",
         "2026-01-13 19:43:23",
         "fine_tunings/unsloth/finetune_mistral_7b",
         "mistral:7b",
         "finetune_mistral_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.8136",
         "0.8426574739132795",
         "0.8136",
         "0.8228481049114522",
         "21.34499884900564",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.9125",
         "0.7877697841726619",
         "0.8455598455598455",
         "278.0",
         "0.8050847457627118",
         "0.8050847457627118",
         "0.8050847457627118",
         "118.0",
         "0.912087912087912",
         "0.6240601503759399",
         "0.7410714285714286",
         "133.0",
         "0.9313304721030042",
         "0.9273504273504274",
         "0.9293361884368307",
         "234.0",
         "0.7654320987654321",
         "0.8378378378378378",
         "0.8",
         "148.0",
         "0.6203007518796992",
         "0.7568807339449541",
         "0.6818181818181818",
         "218.0",
         "0.9661016949152542",
         "0.9421487603305784",
         "0.9539748953974896",
         "121.0"
        ],
        [
         "4",
         "2026-01-13 19:51:18",
         "fine_tunings/unsloth/finetune_phi3_mini",
         "phi3:mini",
         "finetune_phi3_mini",
         "qwedsacf/competition_math",
         "1250",
         "0.6408",
         "0.7359488903672463",
         "0.6408",
         "0.6694143913155204",
         "27.21838942742644",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.852760736196319",
         "0.5",
         "0.6303854875283447",
         "278.0",
         "0.7433628318584071",
         "0.711864406779661",
         "0.7272727272727273",
         "118.0",
         "0.6605504587155964",
         "0.5413533834586466",
         "0.5950413223140496",
         "133.0",
         "0.6516516516516516",
         "0.9273504273504274",
         "0.7654320987654321",
         "234.0",
         "0.811965811965812",
         "0.6418918918918919",
         "0.7169811320754716",
         "148.0",
         "0.5735294117647058",
         "0.536697247706422",
         "0.5545023696682464",
         "218.0",
         "0.9058823529411764",
         "0.6363636363636364",
         "0.7475728155339806",
         "121.0"
        ],
        [
         "5",
         "2026-01-13 19:57:23",
         "fine_tunings/unsloth/finetune_qwen2_7b",
         "qwen2:7b",
         "finetune_qwen2_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.7528",
         "0.7957585759736263",
         "0.7528",
         "0.7593310771509758",
         "15.286894517027031",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.9269662921348316",
         "0.5935251798561151",
         "0.7236842105263158",
         "278.0",
         "0.8604651162790697",
         "0.6271186440677966",
         "0.7254901960784313",
         "118.0",
         "0.7961165048543689",
         "0.6165413533834586",
         "0.6949152542372882",
         "133.0",
         "0.7697841726618705",
         "0.9145299145299144",
         "0.8359375",
         "234.0",
         "0.8027210884353742",
         "0.7972972972972973",
         "0.8",
         "148.0",
         "0.6013986013986014",
         "0.7889908256880734",
         "0.6825396825396826",
         "218.0",
         "0.8226950354609929",
         "0.9586776859504132",
         "0.8854961832061069",
         "121.0"
        ],
        [
         "6",
         "2026-01-13 20:12:39",
         "unsloth/gemma-7b-bnb-4bit",
         "unsloth/gemma-7b-bnb-4bit",
         "base_model_eval",
         "qwedsacf/competition_math",
         "1250",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "18.833848309652137",
         "0",
         "2",
         "0.0002",
         "16",
         "16",
         "0.0",
         "0.0",
         "0.0",
         "278.0",
         "0.0",
         "0.0",
         "0.0",
         "118.0",
         "0.0",
         "0.0",
         "0.0",
         "133.0",
         "0.0",
         "0.0",
         "0.0",
         "234.0",
         "0.0",
         "0.0",
         "0.0",
         "148.0",
         "0.0",
         "0.0",
         "0.0",
         "218.0",
         "0.0",
         "0.0",
         "0.0",
         "121.0"
        ],
        [
         "7",
         "2026-01-13 20:21:49",
         "unsloth/llama-3-8b-bnb-4bit",
         "unsloth/llama-3-8b-bnb-4bit",
         "base_model_eval",
         "qwedsacf/competition_math",
         "1250",
         "0.3624",
         "0.4795969261689213",
         "0.3624",
         "0.3348384505010046",
         "21.96695565112987",
         "0",
         "2",
         "0.0002",
         "16",
         "16",
         "0.3496932515337423",
         "0.4100719424460431",
         "0.3774834437086092",
         "278.0",
         "0.6301369863013698",
         "0.3898305084745763",
         "0.4816753926701571",
         "118.0",
         "0.4117647058823529",
         "0.4736842105263157",
         "0.4405594405594406",
         "133.0",
         "0.2933753943217665",
         "0.3974358974358974",
         "0.337568058076225",
         "234.0",
         "0.4259259259259259",
         "0.777027027027027",
         "0.5502392344497608",
         "148.0",
         "0.5526315789473685",
         "0.0963302752293578",
         "0.1640625",
         "218.0",
         "1.0",
         "0.0082644628099173",
         "0.0163934426229508",
         "121.0"
        ],
        [
         "8",
         "2026-01-13 20:31:00",
         "unsloth/mistral-7b-v0.3-bnb-4bit",
         "unsloth/mistral-7b-v0.3-bnb-4bit",
         "base_model_eval",
         "qwedsacf/competition_math",
         "1250",
         "0.0336",
         "0.1147061403508772",
         "0.0336",
         "0.0489081633165955",
         "22.41165559827087",
         "0",
         "2",
         "0.0002",
         "16",
         "16",
         "0.0",
         "0.0",
         "0.0",
         "278.0",
         "0.0",
         "0.0",
         "0.0",
         "118.0",
         "0.25",
         "0.0150375939849624",
         "0.0283687943262411",
         "133.0",
         "0.0",
         "0.0",
         "0.0",
         "234.0",
         "0.0",
         "0.0",
         "0.0",
         "148.0",
         "0.1929824561403508",
         "0.1009174311926605",
         "0.1325301204819277",
         "218.0",
         "0.5625",
         "0.1487603305785124",
         "0.2352941176470588",
         "121.0"
        ],
        [
         "9",
         "2026-01-13 20:37:25",
         "unsloth/Phi-3-mini-4k-instruct-bnb-4bit",
         "unsloth/Phi-3-mini-4k-instruct-bnb-4bit",
         "base_model_eval",
         "qwedsacf/competition_math",
         "1250",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "37.97737832783515",
         "0",
         "2",
         "0.0002",
         "16",
         "16",
         "0.0",
         "0.0",
         "0.0",
         "278.0",
         "0.0",
         "0.0",
         "0.0",
         "118.0",
         "0.0",
         "0.0",
         "0.0",
         "133.0",
         "0.0",
         "0.0",
         "0.0",
         "234.0",
         "0.0",
         "0.0",
         "0.0",
         "148.0",
         "0.0",
         "0.0",
         "0.0",
         "218.0",
         "0.0",
         "0.0",
         "0.0",
         "121.0"
        ],
        [
         "10",
         "2026-01-13 20:43:17",
         "unsloth/Qwen2-7B-bnb-4bit",
         "unsloth/Qwen2-7B-bnb-4bit",
         "base_model_eval",
         "qwedsacf/competition_math",
         "1250",
         "0.4448",
         "0.4367307964625896",
         "0.4448",
         "0.4077626036968397",
         "13.626540816288028",
         "0",
         "2",
         "0.0002",
         "16",
         "16",
         "0.4631578947368421",
         "0.4748201438848921",
         "0.4689165186500888",
         "278.0",
         "0.5531914893617021",
         "0.6610169491525424",
         "0.6023166023166023",
         "118.0",
         "0.4698795180722891",
         "0.8796992481203008",
         "0.612565445026178",
         "133.0",
         "0.410377358490566",
         "0.3717948717948718",
         "0.3901345291479821",
         "234.0",
         "0.5248868778280543",
         "0.7837837837837838",
         "0.6287262872628726",
         "148.0",
         "0.5306122448979592",
         "0.1192660550458715",
         "0.1947565543071161",
         "218.0",
         "0.0",
         "0.0",
         "0.0",
         "121.0"
        ]
       ],
       "shape": {
        "columns": 44,
        "rows": 11
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model_path</th>\n",
       "      <th>model_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>lora_rank</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>Algebra_precision</th>\n",
       "      <th>Algebra_recall</th>\n",
       "      <th>Algebra_f1</th>\n",
       "      <th>Algebra_support</th>\n",
       "      <th>Counting &amp; Probability_precision</th>\n",
       "      <th>Counting &amp; Probability_recall</th>\n",
       "      <th>Counting &amp; Probability_f1</th>\n",
       "      <th>Counting &amp; Probability_support</th>\n",
       "      <th>Geometry_precision</th>\n",
       "      <th>Geometry_recall</th>\n",
       "      <th>Geometry_f1</th>\n",
       "      <th>Geometry_support</th>\n",
       "      <th>Intermediate Algebra_precision</th>\n",
       "      <th>Intermediate Algebra_recall</th>\n",
       "      <th>Intermediate Algebra_f1</th>\n",
       "      <th>Intermediate Algebra_support</th>\n",
       "      <th>Number Theory_precision</th>\n",
       "      <th>Number Theory_recall</th>\n",
       "      <th>Number Theory_f1</th>\n",
       "      <th>Number Theory_support</th>\n",
       "      <th>Prealgebra_precision</th>\n",
       "      <th>Prealgebra_recall</th>\n",
       "      <th>Prealgebra_f1</th>\n",
       "      <th>Prealgebra_support</th>\n",
       "      <th>Precalculus_precision</th>\n",
       "      <th>Precalculus_recall</th>\n",
       "      <th>Precalculus_f1</th>\n",
       "      <th>Precalculus_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-13 18:22:25</td>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>21.270479</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.762611</td>\n",
       "      <td>0.924460</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.593985</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.955224</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.756906</td>\n",
       "      <td>0.925676</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.481651</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-13 19:23:12</td>\n",
       "      <td>fine_tunings/unsloth/finetune_gemma_7b</td>\n",
       "      <td>gemma:7b</td>\n",
       "      <td>finetune_gemma_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.265760</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-13 19:33:17</td>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>21.294834</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.762611</td>\n",
       "      <td>0.924460</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.593985</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.955224</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.756906</td>\n",
       "      <td>0.925676</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.481651</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-13 19:43:23</td>\n",
       "      <td>fine_tunings/unsloth/finetune_mistral_7b</td>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.842657</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.822848</td>\n",
       "      <td>21.344999</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.787770</td>\n",
       "      <td>0.845560</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.931330</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.620301</td>\n",
       "      <td>0.756881</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.942149</td>\n",
       "      <td>0.953975</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-13 19:51:18</td>\n",
       "      <td>fine_tunings/unsloth/finetune_phi3_mini</td>\n",
       "      <td>phi3:mini</td>\n",
       "      <td>finetune_phi3_mini</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.735949</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.669414</td>\n",
       "      <td>27.218389</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.852761</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.630385</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.660550</td>\n",
       "      <td>0.541353</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.651652</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.641892</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.536697</td>\n",
       "      <td>0.554502</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.747573</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-01-13 19:57:23</td>\n",
       "      <td>fine_tunings/unsloth/finetune_qwen2_7b</td>\n",
       "      <td>qwen2:7b</td>\n",
       "      <td>finetune_qwen2_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.795759</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.759331</td>\n",
       "      <td>15.286895</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.616541</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.601399</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-01-13 20:12:39</td>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.833848</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-01-13 20:21:49</td>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.479597</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>21.966956</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.349693</td>\n",
       "      <td>0.410072</td>\n",
       "      <td>0.377483</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.630137</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.440559</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.293375</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.337568</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.096330</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026-01-13 20:31:00</td>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.114706</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>22.411656</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.100917</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026-01-13 20:37:25</td>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.977378</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2026-01-13 20:43:17</td>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.436731</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.407763</td>\n",
       "      <td>13.626541</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.474820</td>\n",
       "      <td>0.468917</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.602317</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.469880</td>\n",
       "      <td>0.879699</td>\n",
       "      <td>0.612565</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.410377</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.390135</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.524887</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.628726</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.194757</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              timestamp                                model_path  \\\n",
       "0   2026-01-13 18:22:25   fine_tunings/unsloth/finetune_llama3_8b   \n",
       "1   2026-01-13 19:23:12    fine_tunings/unsloth/finetune_gemma_7b   \n",
       "2   2026-01-13 19:33:17   fine_tunings/unsloth/finetune_llama3_8b   \n",
       "3   2026-01-13 19:43:23  fine_tunings/unsloth/finetune_mistral_7b   \n",
       "4   2026-01-13 19:51:18   fine_tunings/unsloth/finetune_phi3_mini   \n",
       "5   2026-01-13 19:57:23    fine_tunings/unsloth/finetune_qwen2_7b   \n",
       "6   2026-01-13 20:12:39                 unsloth/gemma-7b-bnb-4bit   \n",
       "7   2026-01-13 20:21:49               unsloth/llama-3-8b-bnb-4bit   \n",
       "8   2026-01-13 20:31:00          unsloth/mistral-7b-v0.3-bnb-4bit   \n",
       "9   2026-01-13 20:37:25   unsloth/Phi-3-mini-4k-instruct-bnb-4bit   \n",
       "10  2026-01-13 20:43:17                 unsloth/Qwen2-7B-bnb-4bit   \n",
       "\n",
       "                                 model_name             run_name  \\\n",
       "0                                 llama3:8b   finetune_llama3_8b   \n",
       "1                                  gemma:7b    finetune_gemma_7b   \n",
       "2                                 llama3:8b   finetune_llama3_8b   \n",
       "3                                mistral:7b  finetune_mistral_7b   \n",
       "4                                 phi3:mini   finetune_phi3_mini   \n",
       "5                                  qwen2:7b    finetune_qwen2_7b   \n",
       "6                 unsloth/gemma-7b-bnb-4bit      base_model_eval   \n",
       "7               unsloth/llama-3-8b-bnb-4bit      base_model_eval   \n",
       "8          unsloth/mistral-7b-v0.3-bnb-4bit      base_model_eval   \n",
       "9   unsloth/Phi-3-mini-4k-instruct-bnb-4bit      base_model_eval   \n",
       "10                unsloth/Qwen2-7B-bnb-4bit      base_model_eval   \n",
       "\n",
       "                      dataset  test_size  accuracy  precision_weighted  \\\n",
       "0   qwedsacf/competition_math       1250    0.7760            0.827533   \n",
       "1   qwedsacf/competition_math       1250    0.0000            0.000000   \n",
       "2   qwedsacf/competition_math       1250    0.7760            0.827533   \n",
       "3   qwedsacf/competition_math       1250    0.8136            0.842657   \n",
       "4   qwedsacf/competition_math       1250    0.6408            0.735949   \n",
       "5   qwedsacf/competition_math       1250    0.7528            0.795759   \n",
       "6   qwedsacf/competition_math       1250    0.0000            0.000000   \n",
       "7   qwedsacf/competition_math       1250    0.3624            0.479597   \n",
       "8   qwedsacf/competition_math       1250    0.0336            0.114706   \n",
       "9   qwedsacf/competition_math       1250    0.0000            0.000000   \n",
       "10  qwedsacf/competition_math       1250    0.4448            0.436731   \n",
       "\n",
       "    recall_weighted  f1_weighted  tokens_per_sec  num_epochs  batch_size  \\\n",
       "0            0.7760     0.789775       21.270479           3           2   \n",
       "1            0.0000     0.000000       19.265760           3           2   \n",
       "2            0.7760     0.789775       21.294834           3           2   \n",
       "3            0.8136     0.822848       21.344999           3           2   \n",
       "4            0.6408     0.669414       27.218389           3           2   \n",
       "5            0.7528     0.759331       15.286895           3           2   \n",
       "6            0.0000     0.000000       18.833848           0           2   \n",
       "7            0.3624     0.334838       21.966956           0           2   \n",
       "8            0.0336     0.048908       22.411656           0           2   \n",
       "9            0.0000     0.000000       37.977378           0           2   \n",
       "10           0.4448     0.407763       13.626541           0           2   \n",
       "\n",
       "    learning_rate  lora_rank  lora_alpha  Algebra_precision  Algebra_recall  \\\n",
       "0          0.0002         16          16           0.762611        0.924460   \n",
       "1          0.0002         16          16           0.000000        0.000000   \n",
       "2          0.0002         16          16           0.762611        0.924460   \n",
       "3          0.0002         16          16           0.912500        0.787770   \n",
       "4          0.0002         16          16           0.852761        0.500000   \n",
       "5          0.0002         16          16           0.926966        0.593525   \n",
       "6          0.0002         16          16           0.000000        0.000000   \n",
       "7          0.0002         16          16           0.349693        0.410072   \n",
       "8          0.0002         16          16           0.000000        0.000000   \n",
       "9          0.0002         16          16           0.000000        0.000000   \n",
       "10         0.0002         16          16           0.463158        0.474820   \n",
       "\n",
       "    Algebra_f1  Algebra_support  Counting & Probability_precision  \\\n",
       "0     0.835772            278.0                          0.785714   \n",
       "1     0.000000            278.0                          0.000000   \n",
       "2     0.835772            278.0                          0.785714   \n",
       "3     0.845560            278.0                          0.805085   \n",
       "4     0.630385            278.0                          0.743363   \n",
       "5     0.723684            278.0                          0.860465   \n",
       "6     0.000000            278.0                          0.000000   \n",
       "7     0.377483            278.0                          0.630137   \n",
       "8     0.000000            278.0                          0.000000   \n",
       "9     0.000000            278.0                          0.000000   \n",
       "10    0.468917            278.0                          0.553191   \n",
       "\n",
       "    Counting & Probability_recall  Counting & Probability_f1  \\\n",
       "0                        0.745763                   0.765217   \n",
       "1                        0.000000                   0.000000   \n",
       "2                        0.745763                   0.765217   \n",
       "3                        0.805085                   0.805085   \n",
       "4                        0.711864                   0.727273   \n",
       "5                        0.627119                   0.725490   \n",
       "6                        0.000000                   0.000000   \n",
       "7                        0.389831                   0.481675   \n",
       "8                        0.000000                   0.000000   \n",
       "9                        0.000000                   0.000000   \n",
       "10                       0.661017                   0.602317   \n",
       "\n",
       "    Counting & Probability_support  Geometry_precision  Geometry_recall  \\\n",
       "0                            118.0            0.822917         0.593985   \n",
       "1                            118.0            0.000000         0.000000   \n",
       "2                            118.0            0.822917         0.593985   \n",
       "3                            118.0            0.912088         0.624060   \n",
       "4                            118.0            0.660550         0.541353   \n",
       "5                            118.0            0.796117         0.616541   \n",
       "6                            118.0            0.000000         0.000000   \n",
       "7                            118.0            0.411765         0.473684   \n",
       "8                            118.0            0.250000         0.015038   \n",
       "9                            118.0            0.000000         0.000000   \n",
       "10                           118.0            0.469880         0.879699   \n",
       "\n",
       "    Geometry_f1  Geometry_support  Intermediate Algebra_precision  \\\n",
       "0      0.689956             133.0                        0.955224   \n",
       "1      0.000000             133.0                        0.000000   \n",
       "2      0.689956             133.0                        0.955224   \n",
       "3      0.741071             133.0                        0.931330   \n",
       "4      0.595041             133.0                        0.651652   \n",
       "5      0.694915             133.0                        0.769784   \n",
       "6      0.000000             133.0                        0.000000   \n",
       "7      0.440559             133.0                        0.293375   \n",
       "8      0.028369             133.0                        0.000000   \n",
       "9      0.000000             133.0                        0.000000   \n",
       "10     0.612565             133.0                        0.410377   \n",
       "\n",
       "    Intermediate Algebra_recall  Intermediate Algebra_f1  \\\n",
       "0                      0.820513                 0.882759   \n",
       "1                      0.000000                 0.000000   \n",
       "2                      0.820513                 0.882759   \n",
       "3                      0.927350                 0.929336   \n",
       "4                      0.927350                 0.765432   \n",
       "5                      0.914530                 0.835938   \n",
       "6                      0.000000                 0.000000   \n",
       "7                      0.397436                 0.337568   \n",
       "8                      0.000000                 0.000000   \n",
       "9                      0.000000                 0.000000   \n",
       "10                     0.371795                 0.390135   \n",
       "\n",
       "    Intermediate Algebra_support  Number Theory_precision  \\\n",
       "0                          234.0                 0.756906   \n",
       "1                          234.0                 0.000000   \n",
       "2                          234.0                 0.756906   \n",
       "3                          234.0                 0.765432   \n",
       "4                          234.0                 0.811966   \n",
       "5                          234.0                 0.802721   \n",
       "6                          234.0                 0.000000   \n",
       "7                          234.0                 0.425926   \n",
       "8                          234.0                 0.000000   \n",
       "9                          234.0                 0.000000   \n",
       "10                         234.0                 0.524887   \n",
       "\n",
       "    Number Theory_recall  Number Theory_f1  Number Theory_support  \\\n",
       "0               0.925676          0.832827                  148.0   \n",
       "1               0.000000          0.000000                  148.0   \n",
       "2               0.925676          0.832827                  148.0   \n",
       "3               0.837838          0.800000                  148.0   \n",
       "4               0.641892          0.716981                  148.0   \n",
       "5               0.797297          0.800000                  148.0   \n",
       "6               0.000000          0.000000                  148.0   \n",
       "7               0.777027          0.550239                  148.0   \n",
       "8               0.000000          0.000000                  148.0   \n",
       "9               0.000000          0.000000                  148.0   \n",
       "10              0.783784          0.628726                  148.0   \n",
       "\n",
       "    Prealgebra_precision  Prealgebra_recall  Prealgebra_f1  \\\n",
       "0               0.783582           0.481651       0.596591   \n",
       "1               0.000000           0.000000       0.000000   \n",
       "2               0.783582           0.481651       0.596591   \n",
       "3               0.620301           0.756881       0.681818   \n",
       "4               0.573529           0.536697       0.554502   \n",
       "5               0.601399           0.788991       0.682540   \n",
       "6               0.000000           0.000000       0.000000   \n",
       "7               0.552632           0.096330       0.164062   \n",
       "8               0.192982           0.100917       0.132530   \n",
       "9               0.000000           0.000000       0.000000   \n",
       "10              0.530612           0.119266       0.194757   \n",
       "\n",
       "    Prealgebra_support  Precalculus_precision  Precalculus_recall  \\\n",
       "0                218.0               0.941176            0.925620   \n",
       "1                218.0               0.000000            0.000000   \n",
       "2                218.0               0.941176            0.925620   \n",
       "3                218.0               0.966102            0.942149   \n",
       "4                218.0               0.905882            0.636364   \n",
       "5                218.0               0.822695            0.958678   \n",
       "6                218.0               0.000000            0.000000   \n",
       "7                218.0               1.000000            0.008264   \n",
       "8                218.0               0.562500            0.148760   \n",
       "9                218.0               0.000000            0.000000   \n",
       "10               218.0               0.000000            0.000000   \n",
       "\n",
       "    Precalculus_f1  Precalculus_support  \n",
       "0         0.933333                121.0  \n",
       "1         0.000000                121.0  \n",
       "2         0.933333                121.0  \n",
       "3         0.953975                121.0  \n",
       "4         0.747573                121.0  \n",
       "5         0.885496                121.0  \n",
       "6         0.000000                121.0  \n",
       "7         0.016393                121.0  \n",
       "8         0.235294                121.0  \n",
       "9         0.000000                121.0  \n",
       "10        0.000000                121.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load results (assuming notebook is in notebooks/ and csv is in root)\n",
    "try:\n",
    "    df = pd.read_csv('../fine_tunings/experiments.csv')\n",
    "except FileNotFoundError:\n",
    "    # Fallback if running from root\n",
    "    df = pd.read_csv('experiments.csv')\n",
    "\n",
    "print(f\"Total experiments: {len(df)}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "test_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1_weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision_weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall_weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tokens_per_sec",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e323c4d4-38f5-4f28-b220-497200ee3bda",
       "rows": [
        [
         "3",
         "fine_tunings/unsloth/finetune_mistral_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.8136",
         "0.8228481049114522",
         "0.8426574739132795",
         "0.8136",
         "21.34499884900564"
        ],
        [
         "0",
         "fine_tunings/unsloth/finetune_llama3_8b",
         "qwedsacf/competition_math",
         "1250",
         "0.776",
         "0.7897748697439604",
         "0.8275326984558665",
         "0.776",
         "21.270479256307315"
        ],
        [
         "2",
         "fine_tunings/unsloth/finetune_llama3_8b",
         "qwedsacf/competition_math",
         "1250",
         "0.776",
         "0.7897748697439604",
         "0.8275326984558665",
         "0.776",
         "21.29483373828777"
        ],
        [
         "5",
         "fine_tunings/unsloth/finetune_qwen2_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.7528",
         "0.7593310771509758",
         "0.7957585759736263",
         "0.7528",
         "15.286894517027031"
        ],
        [
         "4",
         "fine_tunings/unsloth/finetune_phi3_mini",
         "qwedsacf/competition_math",
         "1250",
         "0.6408",
         "0.6694143913155204",
         "0.7359488903672463",
         "0.6408",
         "27.21838942742644"
        ],
        [
         "10",
         "unsloth/Qwen2-7B-bnb-4bit",
         "qwedsacf/competition_math",
         "1250",
         "0.4448",
         "0.4077626036968397",
         "0.4367307964625896",
         "0.4448",
         "13.626540816288028"
        ],
        [
         "7",
         "unsloth/llama-3-8b-bnb-4bit",
         "qwedsacf/competition_math",
         "1250",
         "0.3624",
         "0.3348384505010046",
         "0.4795969261689213",
         "0.3624",
         "21.96695565112987"
        ],
        [
         "8",
         "unsloth/mistral-7b-v0.3-bnb-4bit",
         "qwedsacf/competition_math",
         "1250",
         "0.0336",
         "0.0489081633165955",
         "0.1147061403508772",
         "0.0336",
         "22.41165559827087"
        ],
        [
         "1",
         "fine_tunings/unsloth/finetune_gemma_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "19.265760125510777"
        ],
        [
         "6",
         "unsloth/gemma-7b-bnb-4bit",
         "qwedsacf/competition_math",
         "1250",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "18.833848309652137"
        ],
        [
         "9",
         "unsloth/Phi-3-mini-4k-instruct-bnb-4bit",
         "qwedsacf/competition_math",
         "1250",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "37.97737832783515"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 11
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine_tunings/unsloth/finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.822848</td>\n",
       "      <td>0.842657</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>21.344999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>21.270479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>21.294834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fine_tunings/unsloth/finetune_qwen2_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.759331</td>\n",
       "      <td>0.795759</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>15.286895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fine_tunings/unsloth/finetune_phi3_mini</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.669414</td>\n",
       "      <td>0.735949</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>27.218389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.407763</td>\n",
       "      <td>0.436731</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>13.626541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>0.479597</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>21.966956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>0.114706</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>22.411656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fine_tunings/unsloth/finetune_gemma_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>19.265760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>18.833848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>37.977378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model_path                    dataset  \\\n",
       "3   fine_tunings/unsloth/finetune_mistral_7b  qwedsacf/competition_math   \n",
       "0    fine_tunings/unsloth/finetune_llama3_8b  qwedsacf/competition_math   \n",
       "2    fine_tunings/unsloth/finetune_llama3_8b  qwedsacf/competition_math   \n",
       "5     fine_tunings/unsloth/finetune_qwen2_7b  qwedsacf/competition_math   \n",
       "4    fine_tunings/unsloth/finetune_phi3_mini  qwedsacf/competition_math   \n",
       "10                 unsloth/Qwen2-7B-bnb-4bit  qwedsacf/competition_math   \n",
       "7                unsloth/llama-3-8b-bnb-4bit  qwedsacf/competition_math   \n",
       "8           unsloth/mistral-7b-v0.3-bnb-4bit  qwedsacf/competition_math   \n",
       "1     fine_tunings/unsloth/finetune_gemma_7b  qwedsacf/competition_math   \n",
       "6                  unsloth/gemma-7b-bnb-4bit  qwedsacf/competition_math   \n",
       "9    unsloth/Phi-3-mini-4k-instruct-bnb-4bit  qwedsacf/competition_math   \n",
       "\n",
       "    test_size  accuracy  f1_weighted  precision_weighted  recall_weighted  \\\n",
       "3        1250    0.8136     0.822848            0.842657           0.8136   \n",
       "0        1250    0.7760     0.789775            0.827533           0.7760   \n",
       "2        1250    0.7760     0.789775            0.827533           0.7760   \n",
       "5        1250    0.7528     0.759331            0.795759           0.7528   \n",
       "4        1250    0.6408     0.669414            0.735949           0.6408   \n",
       "10       1250    0.4448     0.407763            0.436731           0.4448   \n",
       "7        1250    0.3624     0.334838            0.479597           0.3624   \n",
       "8        1250    0.0336     0.048908            0.114706           0.0336   \n",
       "1        1250    0.0000     0.000000            0.000000           0.0000   \n",
       "6        1250    0.0000     0.000000            0.000000           0.0000   \n",
       "9        1250    0.0000     0.000000            0.000000           0.0000   \n",
       "\n",
       "    tokens_per_sec  \n",
       "3        21.344999  \n",
       "0        21.270479  \n",
       "2        21.294834  \n",
       "5        15.286895  \n",
       "4        27.218389  \n",
       "10       13.626541  \n",
       "7        21.966956  \n",
       "8        22.411656  \n",
       "1        19.265760  \n",
       "6        18.833848  \n",
       "9        37.977378  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key metrics comparison\n",
    "summary_cols = ['model_path', 'dataset', 'test_size', 'accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted', 'tokens_per_sec']\n",
    "available_cols = [c for c in summary_cols if c in df.columns]\n",
    "df[available_cols].sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 per-class F1 columns.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "test_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Algebra",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Counting & Probability",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Geometry",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Intermediate Algebra",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number Theory",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Prealgebra",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precalculus",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ddf2f534-6be0-48c3-a955-da75e19deb07",
       "rows": [
        [
         "0",
         "fine_tunings/unsloth/finetune_llama3_8b",
         "qwedsacf/competition_math",
         "1250",
         "0.8357723577235773",
         "0.7652173913043478",
         "0.6899563318777293",
         "0.8827586206896552",
         "0.8328267477203647",
         "0.5965909090909091",
         "0.9333333333333332"
        ],
        [
         "1",
         "fine_tunings/unsloth/finetune_gemma_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "2",
         "fine_tunings/unsloth/finetune_llama3_8b",
         "qwedsacf/competition_math",
         "1250",
         "0.8357723577235773",
         "0.7652173913043478",
         "0.6899563318777293",
         "0.8827586206896552",
         "0.8328267477203647",
         "0.5965909090909091",
         "0.9333333333333332"
        ],
        [
         "3",
         "fine_tunings/unsloth/finetune_mistral_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.8455598455598455",
         "0.8050847457627118",
         "0.7410714285714286",
         "0.9293361884368307",
         "0.8",
         "0.6818181818181818",
         "0.9539748953974896"
        ],
        [
         "4",
         "fine_tunings/unsloth/finetune_phi3_mini",
         "qwedsacf/competition_math",
         "1250",
         "0.6303854875283447",
         "0.7272727272727273",
         "0.5950413223140496",
         "0.7654320987654321",
         "0.7169811320754716",
         "0.5545023696682464",
         "0.7475728155339806"
        ],
        [
         "5",
         "fine_tunings/unsloth/finetune_qwen2_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.7236842105263158",
         "0.7254901960784313",
         "0.6949152542372882",
         "0.8359375",
         "0.8",
         "0.6825396825396826",
         "0.8854961832061069"
        ],
        [
         "6",
         "unsloth/gemma-7b-bnb-4bit",
         "qwedsacf/competition_math",
         "1250",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "7",
         "unsloth/llama-3-8b-bnb-4bit",
         "qwedsacf/competition_math",
         "1250",
         "0.3774834437086092",
         "0.4816753926701571",
         "0.4405594405594406",
         "0.337568058076225",
         "0.5502392344497608",
         "0.1640625",
         "0.0163934426229508"
        ],
        [
         "8",
         "unsloth/mistral-7b-v0.3-bnb-4bit",
         "qwedsacf/competition_math",
         "1250",
         "0.0",
         "0.0",
         "0.0283687943262411",
         "0.0",
         "0.0",
         "0.1325301204819277",
         "0.2352941176470588"
        ],
        [
         "9",
         "unsloth/Phi-3-mini-4k-instruct-bnb-4bit",
         "qwedsacf/competition_math",
         "1250",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "10",
         "unsloth/Qwen2-7B-bnb-4bit",
         "qwedsacf/competition_math",
         "1250",
         "0.4689165186500888",
         "0.6023166023166023",
         "0.612565445026178",
         "0.3901345291479821",
         "0.6287262872628726",
         "0.1947565543071161",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 11
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>Algebra</th>\n",
       "      <th>Counting &amp; Probability</th>\n",
       "      <th>Geometry</th>\n",
       "      <th>Intermediate Algebra</th>\n",
       "      <th>Number Theory</th>\n",
       "      <th>Prealgebra</th>\n",
       "      <th>Precalculus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fine_tunings/unsloth/finetune_gemma_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine_tunings/unsloth/finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.845560</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.953975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fine_tunings/unsloth/finetune_phi3_mini</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.630385</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.554502</td>\n",
       "      <td>0.747573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fine_tunings/unsloth/finetune_qwen2_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.885496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.377483</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>0.440559</td>\n",
       "      <td>0.337568</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.016393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.468917</td>\n",
       "      <td>0.602317</td>\n",
       "      <td>0.612565</td>\n",
       "      <td>0.390135</td>\n",
       "      <td>0.628726</td>\n",
       "      <td>0.194757</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model_path                    dataset  \\\n",
       "0    fine_tunings/unsloth/finetune_llama3_8b  qwedsacf/competition_math   \n",
       "1     fine_tunings/unsloth/finetune_gemma_7b  qwedsacf/competition_math   \n",
       "2    fine_tunings/unsloth/finetune_llama3_8b  qwedsacf/competition_math   \n",
       "3   fine_tunings/unsloth/finetune_mistral_7b  qwedsacf/competition_math   \n",
       "4    fine_tunings/unsloth/finetune_phi3_mini  qwedsacf/competition_math   \n",
       "5     fine_tunings/unsloth/finetune_qwen2_7b  qwedsacf/competition_math   \n",
       "6                  unsloth/gemma-7b-bnb-4bit  qwedsacf/competition_math   \n",
       "7                unsloth/llama-3-8b-bnb-4bit  qwedsacf/competition_math   \n",
       "8           unsloth/mistral-7b-v0.3-bnb-4bit  qwedsacf/competition_math   \n",
       "9    unsloth/Phi-3-mini-4k-instruct-bnb-4bit  qwedsacf/competition_math   \n",
       "10                 unsloth/Qwen2-7B-bnb-4bit  qwedsacf/competition_math   \n",
       "\n",
       "    test_size   Algebra  Counting & Probability  Geometry  \\\n",
       "0        1250  0.835772                0.765217  0.689956   \n",
       "1        1250  0.000000                0.000000  0.000000   \n",
       "2        1250  0.835772                0.765217  0.689956   \n",
       "3        1250  0.845560                0.805085  0.741071   \n",
       "4        1250  0.630385                0.727273  0.595041   \n",
       "5        1250  0.723684                0.725490  0.694915   \n",
       "6        1250  0.000000                0.000000  0.000000   \n",
       "7        1250  0.377483                0.481675  0.440559   \n",
       "8        1250  0.000000                0.000000  0.028369   \n",
       "9        1250  0.000000                0.000000  0.000000   \n",
       "10       1250  0.468917                0.602317  0.612565   \n",
       "\n",
       "    Intermediate Algebra  Number Theory  Prealgebra  Precalculus  \n",
       "0               0.882759       0.832827    0.596591     0.933333  \n",
       "1               0.000000       0.000000    0.000000     0.000000  \n",
       "2               0.882759       0.832827    0.596591     0.933333  \n",
       "3               0.929336       0.800000    0.681818     0.953975  \n",
       "4               0.765432       0.716981    0.554502     0.747573  \n",
       "5               0.835938       0.800000    0.682540     0.885496  \n",
       "6               0.000000       0.000000    0.000000     0.000000  \n",
       "7               0.337568       0.550239    0.164062     0.016393  \n",
       "8               0.000000       0.000000    0.132530     0.235294  \n",
       "9               0.000000       0.000000    0.000000     0.000000  \n",
       "10              0.390135       0.628726    0.194757     0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract per-class F1 columns\n",
    "f1_cols = [c for c in df.columns if c.endswith('_f1') and c != 'f1_weighted']\n",
    "print(f\"Found {len(f1_cols)} per-class F1 columns.\")\n",
    "if f1_cols:\n",
    "    display_df = df[['model_path', 'dataset', 'test_size',] + f1_cols].copy()\n",
    "    display_df.columns = ['model_path', 'dataset', 'test_size',] + [c.replace('_f1', '') for c in f1_cols]\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "num_epochs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "batch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lora_rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lora_alpha",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "8c34f348-ea39-4a2f-a447-4d620ecb709b",
       "rows": [
        [
         "0",
         "fine_tunings/unsloth/finetune_llama3_8b",
         "llama3:8b",
         "3",
         "2",
         "0.0002",
         "16",
         "16"
        ],
        [
         "1",
         "fine_tunings/unsloth/finetune_gemma_7b",
         "gemma:7b",
         "3",
         "2",
         "0.0002",
         "16",
         "16"
        ],
        [
         "2",
         "fine_tunings/unsloth/finetune_llama3_8b",
         "llama3:8b",
         "3",
         "2",
         "0.0002",
         "16",
         "16"
        ],
        [
         "3",
         "fine_tunings/unsloth/finetune_mistral_7b",
         "mistral:7b",
         "3",
         "2",
         "0.0002",
         "16",
         "16"
        ],
        [
         "4",
         "fine_tunings/unsloth/finetune_phi3_mini",
         "phi3:mini",
         "3",
         "2",
         "0.0002",
         "16",
         "16"
        ],
        [
         "5",
         "fine_tunings/unsloth/finetune_qwen2_7b",
         "qwen2:7b",
         "3",
         "2",
         "0.0002",
         "16",
         "16"
        ],
        [
         "6",
         "unsloth/gemma-7b-bnb-4bit",
         "unsloth/gemma-7b-bnb-4bit",
         "0",
         "2",
         "0.0002",
         "16",
         "16"
        ],
        [
         "7",
         "unsloth/llama-3-8b-bnb-4bit",
         "unsloth/llama-3-8b-bnb-4bit",
         "0",
         "2",
         "0.0002",
         "16",
         "16"
        ],
        [
         "8",
         "unsloth/mistral-7b-v0.3-bnb-4bit",
         "unsloth/mistral-7b-v0.3-bnb-4bit",
         "0",
         "2",
         "0.0002",
         "16",
         "16"
        ],
        [
         "9",
         "unsloth/Phi-3-mini-4k-instruct-bnb-4bit",
         "unsloth/Phi-3-mini-4k-instruct-bnb-4bit",
         "0",
         "2",
         "0.0002",
         "16",
         "16"
        ],
        [
         "10",
         "unsloth/Qwen2-7B-bnb-4bit",
         "unsloth/Qwen2-7B-bnb-4bit",
         "0",
         "2",
         "0.0002",
         "16",
         "16"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 11
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>model_name</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>lora_rank</th>\n",
       "      <th>lora_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fine_tunings/unsloth/finetune_gemma_7b</td>\n",
       "      <td>gemma:7b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine_tunings/unsloth/finetune_llama3_8b</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine_tunings/unsloth/finetune_mistral_7b</td>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fine_tunings/unsloth/finetune_phi3_mini</td>\n",
       "      <td>phi3:mini</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fine_tunings/unsloth/finetune_qwen2_7b</td>\n",
       "      <td>qwen2:7b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model_path  \\\n",
       "0    fine_tunings/unsloth/finetune_llama3_8b   \n",
       "1     fine_tunings/unsloth/finetune_gemma_7b   \n",
       "2    fine_tunings/unsloth/finetune_llama3_8b   \n",
       "3   fine_tunings/unsloth/finetune_mistral_7b   \n",
       "4    fine_tunings/unsloth/finetune_phi3_mini   \n",
       "5     fine_tunings/unsloth/finetune_qwen2_7b   \n",
       "6                  unsloth/gemma-7b-bnb-4bit   \n",
       "7                unsloth/llama-3-8b-bnb-4bit   \n",
       "8           unsloth/mistral-7b-v0.3-bnb-4bit   \n",
       "9    unsloth/Phi-3-mini-4k-instruct-bnb-4bit   \n",
       "10                 unsloth/Qwen2-7B-bnb-4bit   \n",
       "\n",
       "                                 model_name  num_epochs  batch_size  \\\n",
       "0                                 llama3:8b           3           2   \n",
       "1                                  gemma:7b           3           2   \n",
       "2                                 llama3:8b           3           2   \n",
       "3                                mistral:7b           3           2   \n",
       "4                                 phi3:mini           3           2   \n",
       "5                                  qwen2:7b           3           2   \n",
       "6                 unsloth/gemma-7b-bnb-4bit           0           2   \n",
       "7               unsloth/llama-3-8b-bnb-4bit           0           2   \n",
       "8          unsloth/mistral-7b-v0.3-bnb-4bit           0           2   \n",
       "9   unsloth/Phi-3-mini-4k-instruct-bnb-4bit           0           2   \n",
       "10                unsloth/Qwen2-7B-bnb-4bit           0           2   \n",
       "\n",
       "    learning_rate  lora_rank  lora_alpha  \n",
       "0          0.0002         16          16  \n",
       "1          0.0002         16          16  \n",
       "2          0.0002         16          16  \n",
       "3          0.0002         16          16  \n",
       "4          0.0002         16          16  \n",
       "5          0.0002         16          16  \n",
       "6          0.0002         16          16  \n",
       "7          0.0002         16          16  \n",
       "8          0.0002         16          16  \n",
       "9          0.0002         16          16  \n",
       "10         0.0002         16          16  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config columns\n",
    "config_cols = ['model_path', 'model_name', 'num_epochs', 'batch_size', 'learning_rate', 'lora_rank', 'lora_alpha']\n",
    "available_config = [c for c in config_cols if c in df.columns]\n",
    "df[available_config]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Finetune",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1_Weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Avg_Latency_Seconds",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Task",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "598185f8-8373-4f77-8424-049f23d6fca2",
       "rows": [
        [
         "1",
         "qwen2:7b",
         "classification head classification finetune",
         "0.8512",
         "0.8496990525356527",
         "0.14411031589508055",
         "FineTuned classification"
        ],
        [
         "2",
         "llama3:8b",
         "classification head classification finetune",
         "0.8328",
         "0.8325535700454885",
         "0.15648587512969972",
         "FineTuned classification"
        ],
        [
         "5",
         "mistral:7b",
         "generative classification finetune",
         "0.8136",
         "0.8228481049114522",
         "0.468493817720016",
         "FineTuned classification"
        ],
        [
         "0",
         "mistral:7b",
         "classification head classification finetune",
         "0.808",
         "0.8053752685154599",
         "0.17734591369628908",
         "FineTuned classification"
        ],
        [
         "4",
         "llama3:8b",
         "generative classification finetune",
         "0.776",
         "0.7897748697439604",
         "0.4695974677660977",
         "FineTuned classification"
        ],
        [
         "7",
         "qwen2:7b",
         "generative classification finetune",
         "0.7528",
         "0.7593310771509758",
         "0.29290448723988416",
         "FineTuned classification"
        ],
        [
         "6",
         "phi3:mini",
         "generative classification finetune",
         "0.6408",
         "0.6694143913155204",
         "0.3673986672379506",
         "FineTuned classification"
        ],
        [
         "12",
         "unsloth/Qwen2-7B-bnb-4bit",
         "generative classification finetune",
         "0.4448",
         "0.40776260369683975",
         "0.2100606484500106",
         "FineTuned classification"
        ],
        [
         "9",
         "unsloth/llama-3-8b-bnb-4bit",
         "generative classification finetune",
         "0.3624",
         "0.33483845050100464",
         "0.42656798460455003",
         "FineTuned classification"
        ],
        [
         "10",
         "unsloth/mistral-7b-v0.3-bnb-4bit",
         "generative classification finetune",
         "0.0336",
         "0.048908163316595546",
         "0.420138528307854",
         "FineTuned classification"
        ],
        [
         "3",
         "gemma:7b",
         "generative classification finetune",
         "0.0",
         "0.0",
         "0.5188894668507125",
         "FineTuned classification"
        ],
        [
         "8",
         "unsloth/gemma-7b-bnb-4bit",
         "None",
         "0.0",
         "0.0",
         "0.4472798050350015",
         "Zero-shot generative classification"
        ],
        [
         "11",
         "unsloth/Phi-3-mini-4k-instruct-bnb-4bit",
         "generative classification finetune",
         "0.0",
         "0.0",
         "0.2633146478326177",
         "FineTuned classification"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 13
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Finetune</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Weighted</th>\n",
       "      <th>Avg_Latency_Seconds</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qwen2:7b</td>\n",
       "      <td>classification head classification finetune</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.849699</td>\n",
       "      <td>0.144110</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>classification head classification finetune</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.832554</td>\n",
       "      <td>0.156486</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.822848</td>\n",
       "      <td>0.468494</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>classification head classification finetune</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.805375</td>\n",
       "      <td>0.177346</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>0.469597</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qwen2:7b</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.759331</td>\n",
       "      <td>0.292904</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phi3:mini</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.669414</td>\n",
       "      <td>0.367399</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>unsloth/Qwen2-7B-bnb-4bit</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.407763</td>\n",
       "      <td>0.210061</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unsloth/llama-3-8b-bnb-4bit</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>0.426568</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unsloth/mistral-7b-v0.3-bnb-4bit</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>0.420139</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma:7b</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.518889</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unsloth/gemma-7b-bnb-4bit</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447280</td>\n",
       "      <td>Zero-shot generative classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>unsloth/Phi-3-mini-4k-instruct-bnb-4bit</td>\n",
       "      <td>generative classification finetune</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263315</td>\n",
       "      <td>FineTuned classification</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Model  \\\n",
       "1                                  qwen2:7b   \n",
       "2                                 llama3:8b   \n",
       "5                                mistral:7b   \n",
       "0                                mistral:7b   \n",
       "4                                 llama3:8b   \n",
       "7                                  qwen2:7b   \n",
       "6                                 phi3:mini   \n",
       "12                unsloth/Qwen2-7B-bnb-4bit   \n",
       "9               unsloth/llama-3-8b-bnb-4bit   \n",
       "10         unsloth/mistral-7b-v0.3-bnb-4bit   \n",
       "3                                  gemma:7b   \n",
       "8                 unsloth/gemma-7b-bnb-4bit   \n",
       "11  unsloth/Phi-3-mini-4k-instruct-bnb-4bit   \n",
       "\n",
       "                                       Finetune  Accuracy  F1_Weighted  \\\n",
       "1   classification head classification finetune    0.8512     0.849699   \n",
       "2   classification head classification finetune    0.8328     0.832554   \n",
       "5            generative classification finetune    0.8136     0.822848   \n",
       "0   classification head classification finetune    0.8080     0.805375   \n",
       "4            generative classification finetune    0.7760     0.789775   \n",
       "7            generative classification finetune    0.7528     0.759331   \n",
       "6            generative classification finetune    0.6408     0.669414   \n",
       "12           generative classification finetune    0.4448     0.407763   \n",
       "9            generative classification finetune    0.3624     0.334838   \n",
       "10           generative classification finetune    0.0336     0.048908   \n",
       "3            generative classification finetune    0.0000     0.000000   \n",
       "8                                          None    0.0000     0.000000   \n",
       "11           generative classification finetune    0.0000     0.000000   \n",
       "\n",
       "    Avg_Latency_Seconds                                 Task  \n",
       "1              0.144110             FineTuned classification  \n",
       "2              0.156486             FineTuned classification  \n",
       "5              0.468494             FineTuned classification  \n",
       "0              0.177346             FineTuned classification  \n",
       "4              0.469597             FineTuned classification  \n",
       "7              0.292904             FineTuned classification  \n",
       "6              0.367399             FineTuned classification  \n",
       "12             0.210061             FineTuned classification  \n",
       "9              0.426568             FineTuned classification  \n",
       "10             0.420139             FineTuned classification  \n",
       "3              0.518889             FineTuned classification  \n",
       "8              0.447280  Zero-shot generative classification  \n",
       "11             0.263315             FineTuned classification  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Performance by Fine-tune Type:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Finetune",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1_Weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Avg_Latency_Seconds",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "5ea563b6-9027-4979-9a29-0cb7518ef24d",
       "rows": [
        [
         "None",
         "0.0",
         "0.0",
         "0.4472798050350015"
        ],
        [
         "classification head classification finetune",
         "0.8306666666666667",
         "0.8292092970322004",
         "0.15931403490702312"
        ],
        [
         "generative classification finetune",
         "0.42488888888888887",
         "0.425875295626261",
         "0.381929524001077"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Weighted</th>\n",
       "      <th>Avg_Latency_Seconds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finetune</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classification head classification finetune</th>\n",
       "      <td>0.830667</td>\n",
       "      <td>0.829209</td>\n",
       "      <td>0.159314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generative classification finetune</th>\n",
       "      <td>0.424889</td>\n",
       "      <td>0.425875</td>\n",
       "      <td>0.381930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Accuracy  F1_Weighted  \\\n",
       "Finetune                                                             \n",
       "None                                         0.000000     0.000000   \n",
       "classification head classification finetune  0.830667     0.829209   \n",
       "generative classification finetune           0.424889     0.425875   \n",
       "\n",
       "                                             Avg_Latency_Seconds  \n",
       "Finetune                                                          \n",
       "None                                                    0.447280  \n",
       "classification head classification finetune             0.159314  \n",
       "generative classification finetune                      0.381930  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Load evaluation results\n",
    "yaml_path = '../fine_tunings/evaluation_results.yaml'\n",
    "\n",
    "try:\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        eval_data = yaml.safe_load(f)\n",
    "\n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "\n",
    "    # improved display\n",
    "    display_cols = ['Model', 'Finetune', 'Accuracy', 'F1_Weighted', 'Avg_Latency_Seconds', 'Task']\n",
    "    print(\"Evaluation Results:\")\n",
    "    display(eval_df[display_cols].sort_values('Accuracy', ascending=False))\n",
    "\n",
    "    # Grouped analysis\n",
    "    print(\"\\nAverage Performance by Fine-tune Type:\")\n",
    "    display(eval_df.groupby('Finetune')[['Accuracy', 'F1_Weighted', 'Avg_Latency_Seconds']].mean(numeric_only=True))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Could not find file at {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Result\n",
    "\n",
    "Based on the evaluation results:\n",
    "\n",
    "1.  **Discriminative Fine-tuning (Classification Head)**:\n",
    "    *   This approach consistently yields high performance.\n",
    "    *   **Qwen2:7b** is the top performer with **~85.1% accuracy**.\n",
    "    *   **Llama3:8b** (~83.3%) and **Mistral:7b** (~80.8%) also perform very well.\n",
    "    *   Latency is consistently low (~0.15s), making it efficient for pure classification tasks.\n",
    "\n",
    "2.  **Generative Fine-tuning**:\n",
    "    *   Performance is more volatile compared to the classification head approach.\n",
    "    *   **Mistral:7b** adapts best to the generative format for this task, achieving **81.4% accuracy**, which is slightly better than its discriminative counterpart.\n",
    "    *   **Llama3** and **Qwen2** see a drop in performance (77.6% and 75.3% respectively) compared to their classification head versions.\n",
    "    *   Several models (Gemma, and unsloth quantized versions of Phi-3/Mistral) struggled significantly, showing near 0% accuracy. This suggests sensitivity to hyperparameters, quantization, or output formatting in the generative setting.\n",
    "\n",
    "3.  **Overall Conclusion**:\n",
    "    *   For this specific MATH dataset classification task, **Discriminative Fine-tuning** (Class Head) appears to be the more robust and higher-performing strategy for most models, offering better accuracy and lower latency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison Summary:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Model_Normalized",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Zero shot baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SFT (Discriminative)",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "SFT (Generative)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "409498d1-14da-49f7-8cd1-198bd62b2b4b",
       "rows": [
        [
         "gemma:7b",
         "0.0",
         "N/A",
         "0.0"
        ],
        [
         "llama3:8b",
         "0.3624",
         "0.8328",
         "0.776"
        ],
        [
         "mistral:7b",
         "0.0336",
         "0.808",
         "0.8136"
        ],
        [
         "phi3:mini",
         "0.0",
         "N/A",
         "0.6408"
        ],
        [
         "qwen2:7b",
         "0.4448",
         "0.8512",
         "0.7528"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Experiment_Type</th>\n",
       "      <th>Zero shot baseline</th>\n",
       "      <th>SFT (Discriminative)</th>\n",
       "      <th>SFT (Generative)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model_Normalized</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemma:7b</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3:8b</th>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.7760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral:7b</th>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.8136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi3:mini</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.6408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2:7b</th>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.7528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Experiment_Type   Zero shot baseline SFT (Discriminative)  SFT (Generative)\n",
       "Model_Normalized                                                           \n",
       "gemma:7b                      0.0000                  N/A            0.0000\n",
       "llama3:8b                     0.3624               0.8328            0.7760\n",
       "mistral:7b                    0.0336                0.808            0.8136\n",
       "phi3:mini                     0.0000                  N/A            0.6408\n",
       "qwen2:7b                      0.4448               0.8512            0.7528"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define categorization logic\n",
    "def get_experiment_category(row):\n",
    "    model = row.get('Model', '')\n",
    "    ft = row.get('Finetune')\n",
    "    task = row.get('Task')\n",
    "\n",
    "    # unsloth/* models are the base/zero-shot models\n",
    "    if model.startswith('unsloth/'):\n",
    "        return 'Zero shot baseline'\n",
    "    # Check for discriminative\n",
    "    elif ft == 'classification head classification finetune':\n",
    "        return 'SFT (Discriminative)'\n",
    "    # Check for generative\n",
    "    elif ft == 'generative classification finetune':\n",
    "        return 'SFT (Generative)'\n",
    "    # Check for zero shot / base\n",
    "    elif task == 'Zero-shot generative classification' or pd.isna(ft) or ft == 'None':\n",
    "        return 'Zero shot baseline'\n",
    "    return 'Other'\n",
    "\n",
    "# Apply categorization\n",
    "eval_df['Experiment_Type'] = eval_df.apply(get_experiment_category, axis=1)\n",
    "\n",
    "# Normalize model names to associate base models with finetuned ones\n",
    "def normalize_model_name(name):\n",
    "    name_lower = name.lower()\n",
    "    if 'gemma' in name_lower and '7b' in name_lower:\n",
    "        return 'gemma:7b'\n",
    "    if 'llama' in name_lower and '3' in name_lower and '8b' in name_lower:\n",
    "        return 'llama3:8b'\n",
    "    if 'mistral' in name_lower and '7b' in name_lower:\n",
    "        return 'mistral:7b'\n",
    "    if 'phi' in name_lower and '3' in name_lower and 'mini' in name_lower:\n",
    "        return 'phi3:mini'\n",
    "    if 'qwen' in name_lower and '2' in name_lower and '7b' in name_lower:\n",
    "        return 'qwen2:7b'\n",
    "    return name\n",
    "\n",
    "eval_df['Model_Normalized'] = eval_df['Model'].apply(normalize_model_name)\n",
    "\n",
    "# Create pivot table using normalized model names\n",
    "comparison_table = eval_df.pivot_table(\n",
    "    index='Model_Normalized',\n",
    "    columns='Experiment_Type',\n",
    "    values='Accuracy',\n",
    "    aggfunc='max'\n",
    ")\n",
    "\n",
    "# Reorder columns strictly as requested\n",
    "target_cols = ['Zero shot baseline', 'SFT (Discriminative)', 'SFT (Generative)']\n",
    "existing_target_cols = [c for c in target_cols if c in comparison_table.columns]\n",
    "comparison_table = comparison_table[existing_target_cols]\n",
    "\n",
    "# Fill NaNs with \"N/A\"\n",
    "comparison_table = comparison_table.fillna('N/A')\n",
    "\n",
    "print(\"Model Comparison Summary:\")\n",
    "display(comparison_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advancednlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
