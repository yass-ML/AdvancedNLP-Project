{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results Viewer\n",
    "\n",
    "This notebook displays the results from fine-tuning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "run_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "test_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision_weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall_weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1_weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tokens_per_sec",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "num_epochs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "batch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lora_rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lora_alpha",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Algebra_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Algebra_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Algebra_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Algebra_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Counting & Probability_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Counting & Probability_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Counting & Probability_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Counting & Probability_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Geometry_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Geometry_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Geometry_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Geometry_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Intermediate Algebra_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Intermediate Algebra_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Intermediate Algebra_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Intermediate Algebra_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number Theory_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number Theory_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number Theory_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number Theory_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Prealgebra_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Prealgebra_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Prealgebra_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Prealgebra_support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precalculus_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precalculus_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precalculus_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precalculus_support",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7cf81acb-1a37-468e-bdca-4198314e0217",
       "rows": [
        [
         "0",
         "2026-01-04 12:50:25",
         "fine_tunings/finetune_mistral_7b/checkpoints/checkpoint-3750",
         "mistral:7b",
         "finetune_mistral_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.8072",
         "0.8402469026502258",
         "0.8072",
         "0.8192527995833305",
         "21.231163341640222",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.9110169491525424",
         "0.7733812949640287",
         "0.8365758754863813",
         "278.0",
         "0.7786885245901639",
         "0.8050847457627118",
         "0.7916666666666666",
         "118.0",
         "0.8807339449541285",
         "0.7218045112781954",
         "0.7933884297520661",
         "133.0",
         "0.9156118143459916",
         "0.9273504273504274",
         "0.921443736730361",
         "234.0",
         "0.8524590163934426",
         "0.7027027027027027",
         "0.7703703703703704",
         "148.0",
         "0.608058608058608",
         "0.7614678899082569",
         "0.6761710794297352",
         "218.0",
         "0.9508196721311476",
         "0.9586776859504132",
         "0.9547325102880658",
         "121.0"
        ],
        [
         "1",
         "2026-01-04 13:10:49",
         "fine_tunings/finetune_mistral_7b",
         "mistral:7b",
         "finetune_mistral_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.8136",
         "0.8426574739132795",
         "0.8136",
         "0.8228481049114522",
         "20.86332202697934",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.9125",
         "0.7877697841726619",
         "0.8455598455598455",
         "278.0",
         "0.8050847457627118",
         "0.8050847457627118",
         "0.8050847457627118",
         "118.0",
         "0.912087912087912",
         "0.6240601503759399",
         "0.7410714285714286",
         "133.0",
         "0.9313304721030042",
         "0.9273504273504274",
         "0.9293361884368307",
         "234.0",
         "0.7654320987654321",
         "0.8378378378378378",
         "0.8",
         "148.0",
         "0.6203007518796992",
         "0.7568807339449541",
         "0.6818181818181818",
         "218.0",
         "0.9661016949152542",
         "0.9421487603305784",
         "0.9539748953974896",
         "121.0"
        ],
        [
         "2",
         "2026-01-04 13:37:46",
         "fine_tunings/finetune_llama3_8b",
         "llama3:8b",
         "finetune_llama3_8b",
         "qwedsacf/competition_math",
         "1250",
         "0.776",
         "0.8275326984558665",
         "0.776",
         "0.7897748697439604",
         "20.74860686119406",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.7626112759643917",
         "0.9244604316546764",
         "0.8357723577235773",
         "278.0",
         "0.7857142857142857",
         "0.7457627118644068",
         "0.7652173913043478",
         "118.0",
         "0.8229166666666666",
         "0.5939849624060151",
         "0.6899563318777293",
         "133.0",
         "0.9552238805970148",
         "0.8205128205128205",
         "0.8827586206896552",
         "234.0",
         "0.7569060773480663",
         "0.9256756756756755",
         "0.8328267477203647",
         "148.0",
         "0.7835820895522388",
         "0.481651376146789",
         "0.5965909090909091",
         "218.0",
         "0.9411764705882352",
         "0.9256198347107438",
         "0.9333333333333332",
         "121.0"
        ],
        [
         "3",
         "2026-01-04 13:48:44",
         "fine_tunings/finetune_llama3_8b/checkpoints/checkpoint-3750",
         "llama3:8b",
         "finetune_llama3_8b",
         "qwedsacf/competition_math",
         "1250",
         "0.764",
         "0.8296146681458053",
         "0.764",
         "0.783393425662891",
         "20.687743074040046",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.7836990595611285",
         "0.8992805755395683",
         "0.8375209380234506",
         "278.0",
         "0.7777777777777778",
         "0.7711864406779662",
         "0.774468085106383",
         "118.0",
         "0.7830188679245284",
         "0.6240601503759399",
         "0.694560669456067",
         "133.0",
         "0.9840425531914894",
         "0.7905982905982906",
         "0.8767772511848341",
         "234.0",
         "0.7848837209302325",
         "0.9121621621621622",
         "0.84375",
         "148.0",
         "0.7933884297520661",
         "0.4403669724770642",
         "0.5663716814159292",
         "218.0",
         "0.8582089552238806",
         "0.950413223140496",
         "0.9019607843137256",
         "121.0"
        ],
        [
         "4",
         "2026-01-04 14:04:38",
         "meta-llama/Meta-Llama-3-8B",
         "meta-llama/Meta-Llama-3-8B",
         "base_model_eval",
         "qwedsacf/competition_math",
         "1250",
         "0.3624",
         "0.4795969261689213",
         "0.3624",
         "0.3348384505010046",
         "21.537798576295756",
         "0",
         "2",
         "0.0002",
         "16",
         "16",
         "0.3496932515337423",
         "0.4100719424460431",
         "0.3774834437086092",
         "278.0",
         "0.6301369863013698",
         "0.3898305084745763",
         "0.4816753926701571",
         "118.0",
         "0.4117647058823529",
         "0.4736842105263157",
         "0.4405594405594406",
         "133.0",
         "0.2933753943217665",
         "0.3974358974358974",
         "0.337568058076225",
         "234.0",
         "0.4259259259259259",
         "0.777027027027027",
         "0.5502392344497608",
         "148.0",
         "0.5526315789473685",
         "0.0963302752293578",
         "0.1640625",
         "218.0",
         "1.0",
         "0.0082644628099173",
         "0.0163934426229508",
         "121.0"
        ],
        [
         "5",
         "2026-01-04 14:14:33",
         "mistralai/Mistral-7B-v0.3",
         "mistralai/Mistral-7B-v0.3",
         "base_model_eval",
         "qwedsacf/competition_math",
         "1250",
         "0.0336",
         "0.1147061403508772",
         "0.0336",
         "0.0489081633165955",
         "21.94026559229691",
         "0",
         "2",
         "0.0002",
         "16",
         "16",
         "0.0",
         "0.0",
         "0.0",
         "278.0",
         "0.0",
         "0.0",
         "0.0",
         "118.0",
         "0.25",
         "0.0150375939849624",
         "0.0283687943262411",
         "133.0",
         "0.0",
         "0.0",
         "0.0",
         "234.0",
         "0.0",
         "0.0",
         "0.0",
         "148.0",
         "0.1929824561403508",
         "0.1009174311926605",
         "0.1325301204819277",
         "218.0",
         "0.5625",
         "0.1487603305785124",
         "0.2352941176470588",
         "121.0"
        ],
        [
         "6",
         "2026-01-05 15:25:43",
         "fine_tunings/finetune_qwen2_7b/",
         "qwen2:7b",
         "finetune_qwen2_7b",
         "qwedsacf/competition_math",
         "1250",
         "0.7528",
         "0.7957585759736263",
         "0.7528",
         "0.7593310771509758",
         "15.381560798944529",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.9269662921348316",
         "0.5935251798561151",
         "0.7236842105263158",
         "278.0",
         "0.8604651162790697",
         "0.6271186440677966",
         "0.7254901960784313",
         "118.0",
         "0.7961165048543689",
         "0.6165413533834586",
         "0.6949152542372882",
         "133.0",
         "0.7697841726618705",
         "0.9145299145299144",
         "0.8359375",
         "234.0",
         "0.8027210884353742",
         "0.7972972972972973",
         "0.8",
         "148.0",
         "0.6013986013986014",
         "0.7889908256880734",
         "0.6825396825396826",
         "218.0",
         "0.8226950354609929",
         "0.9586776859504132",
         "0.8854961832061069",
         "121.0"
        ],
        [
         "7",
         "2026-01-06 00:23:19",
         "fine_tunings/finetune_phi3_mini",
         "phi3:mini",
         "finetune_phi3_mini",
         "qwedsacf/competition_math",
         "1250",
         "0.6408",
         "0.7359488903672463",
         "0.6408",
         "0.6694143913155204",
         "28.02231106638428",
         "3",
         "2",
         "0.0002",
         "16",
         "16",
         "0.852760736196319",
         "0.5",
         "0.6303854875283447",
         "278.0",
         "0.7433628318584071",
         "0.711864406779661",
         "0.7272727272727273",
         "118.0",
         "0.6605504587155964",
         "0.5413533834586466",
         "0.5950413223140496",
         "133.0",
         "0.6516516516516516",
         "0.9273504273504274",
         "0.7654320987654321",
         "234.0",
         "0.811965811965812",
         "0.6418918918918919",
         "0.7169811320754716",
         "148.0",
         "0.5735294117647058",
         "0.536697247706422",
         "0.5545023696682464",
         "218.0",
         "0.9058823529411764",
         "0.6363636363636364",
         "0.7475728155339806",
         "121.0"
        ]
       ],
       "shape": {
        "columns": 44,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model_path</th>\n",
       "      <th>model_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>lora_rank</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>Algebra_precision</th>\n",
       "      <th>Algebra_recall</th>\n",
       "      <th>Algebra_f1</th>\n",
       "      <th>Algebra_support</th>\n",
       "      <th>Counting &amp; Probability_precision</th>\n",
       "      <th>Counting &amp; Probability_recall</th>\n",
       "      <th>Counting &amp; Probability_f1</th>\n",
       "      <th>Counting &amp; Probability_support</th>\n",
       "      <th>Geometry_precision</th>\n",
       "      <th>Geometry_recall</th>\n",
       "      <th>Geometry_f1</th>\n",
       "      <th>Geometry_support</th>\n",
       "      <th>Intermediate Algebra_precision</th>\n",
       "      <th>Intermediate Algebra_recall</th>\n",
       "      <th>Intermediate Algebra_f1</th>\n",
       "      <th>Intermediate Algebra_support</th>\n",
       "      <th>Number Theory_precision</th>\n",
       "      <th>Number Theory_recall</th>\n",
       "      <th>Number Theory_f1</th>\n",
       "      <th>Number Theory_support</th>\n",
       "      <th>Prealgebra_precision</th>\n",
       "      <th>Prealgebra_recall</th>\n",
       "      <th>Prealgebra_f1</th>\n",
       "      <th>Prealgebra_support</th>\n",
       "      <th>Precalculus_precision</th>\n",
       "      <th>Precalculus_recall</th>\n",
       "      <th>Precalculus_f1</th>\n",
       "      <th>Precalculus_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-04 12:50:25</td>\n",
       "      <td>fine_tunings/finetune_mistral_7b/checkpoints/c...</td>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8072</td>\n",
       "      <td>0.840247</td>\n",
       "      <td>0.8072</td>\n",
       "      <td>0.819253</td>\n",
       "      <td>21.231163</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.911017</td>\n",
       "      <td>0.773381</td>\n",
       "      <td>0.836576</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.915612</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.921444</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.608059</td>\n",
       "      <td>0.761468</td>\n",
       "      <td>0.676171</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.954733</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-04 13:10:49</td>\n",
       "      <td>fine_tunings/finetune_mistral_7b</td>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.842657</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.822848</td>\n",
       "      <td>20.863322</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.787770</td>\n",
       "      <td>0.845560</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.931330</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.620301</td>\n",
       "      <td>0.756881</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.942149</td>\n",
       "      <td>0.953975</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-04 13:37:46</td>\n",
       "      <td>fine_tunings/finetune_llama3_8b</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>20.748607</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.762611</td>\n",
       "      <td>0.924460</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.593985</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.955224</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.756906</td>\n",
       "      <td>0.925676</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.481651</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-04 13:48:44</td>\n",
       "      <td>fine_tunings/finetune_llama3_8b/checkpoints/ch...</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7640</td>\n",
       "      <td>0.829615</td>\n",
       "      <td>0.7640</td>\n",
       "      <td>0.783393</td>\n",
       "      <td>20.687743</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.783699</td>\n",
       "      <td>0.899281</td>\n",
       "      <td>0.837521</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.771186</td>\n",
       "      <td>0.774468</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.783019</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.694561</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.876777</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.784884</td>\n",
       "      <td>0.912162</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>0.440367</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.950413</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-04 14:04:38</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.479597</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>21.537799</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.349693</td>\n",
       "      <td>0.410072</td>\n",
       "      <td>0.377483</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.630137</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.440559</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.293375</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.337568</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.096330</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-01-04 14:14:33</td>\n",
       "      <td>mistralai/Mistral-7B-v0.3</td>\n",
       "      <td>mistralai/Mistral-7B-v0.3</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.114706</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>21.940266</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.100917</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-01-05 15:25:43</td>\n",
       "      <td>fine_tunings/finetune_qwen2_7b/</td>\n",
       "      <td>qwen2:7b</td>\n",
       "      <td>finetune_qwen2_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.795759</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.759331</td>\n",
       "      <td>15.381561</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.616541</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.601399</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-01-06 00:23:19</td>\n",
       "      <td>fine_tunings/finetune_phi3_mini</td>\n",
       "      <td>phi3:mini</td>\n",
       "      <td>finetune_phi3_mini</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.735949</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.669414</td>\n",
       "      <td>28.022311</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.852761</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.630385</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.660550</td>\n",
       "      <td>0.541353</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.651652</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.641892</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.536697</td>\n",
       "      <td>0.554502</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.747573</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                                         model_path  \\\n",
       "0  2026-01-04 12:50:25  fine_tunings/finetune_mistral_7b/checkpoints/c...   \n",
       "1  2026-01-04 13:10:49                   fine_tunings/finetune_mistral_7b   \n",
       "2  2026-01-04 13:37:46                    fine_tunings/finetune_llama3_8b   \n",
       "3  2026-01-04 13:48:44  fine_tunings/finetune_llama3_8b/checkpoints/ch...   \n",
       "4  2026-01-04 14:04:38                         meta-llama/Meta-Llama-3-8B   \n",
       "5  2026-01-04 14:14:33                          mistralai/Mistral-7B-v0.3   \n",
       "6  2026-01-05 15:25:43                    fine_tunings/finetune_qwen2_7b/   \n",
       "7  2026-01-06 00:23:19                    fine_tunings/finetune_phi3_mini   \n",
       "\n",
       "                   model_name             run_name                    dataset  \\\n",
       "0                  mistral:7b  finetune_mistral_7b  qwedsacf/competition_math   \n",
       "1                  mistral:7b  finetune_mistral_7b  qwedsacf/competition_math   \n",
       "2                   llama3:8b   finetune_llama3_8b  qwedsacf/competition_math   \n",
       "3                   llama3:8b   finetune_llama3_8b  qwedsacf/competition_math   \n",
       "4  meta-llama/Meta-Llama-3-8B      base_model_eval  qwedsacf/competition_math   \n",
       "5   mistralai/Mistral-7B-v0.3      base_model_eval  qwedsacf/competition_math   \n",
       "6                    qwen2:7b    finetune_qwen2_7b  qwedsacf/competition_math   \n",
       "7                   phi3:mini   finetune_phi3_mini  qwedsacf/competition_math   \n",
       "\n",
       "   test_size  accuracy  precision_weighted  recall_weighted  f1_weighted  \\\n",
       "0       1250    0.8072            0.840247           0.8072     0.819253   \n",
       "1       1250    0.8136            0.842657           0.8136     0.822848   \n",
       "2       1250    0.7760            0.827533           0.7760     0.789775   \n",
       "3       1250    0.7640            0.829615           0.7640     0.783393   \n",
       "4       1250    0.3624            0.479597           0.3624     0.334838   \n",
       "5       1250    0.0336            0.114706           0.0336     0.048908   \n",
       "6       1250    0.7528            0.795759           0.7528     0.759331   \n",
       "7       1250    0.6408            0.735949           0.6408     0.669414   \n",
       "\n",
       "   tokens_per_sec  num_epochs  batch_size  learning_rate  lora_rank  \\\n",
       "0       21.231163           3           2         0.0002         16   \n",
       "1       20.863322           3           2         0.0002         16   \n",
       "2       20.748607           3           2         0.0002         16   \n",
       "3       20.687743           3           2         0.0002         16   \n",
       "4       21.537799           0           2         0.0002         16   \n",
       "5       21.940266           0           2         0.0002         16   \n",
       "6       15.381561           3           2         0.0002         16   \n",
       "7       28.022311           3           2         0.0002         16   \n",
       "\n",
       "   lora_alpha  Algebra_precision  Algebra_recall  Algebra_f1  Algebra_support  \\\n",
       "0          16           0.911017        0.773381    0.836576            278.0   \n",
       "1          16           0.912500        0.787770    0.845560            278.0   \n",
       "2          16           0.762611        0.924460    0.835772            278.0   \n",
       "3          16           0.783699        0.899281    0.837521            278.0   \n",
       "4          16           0.349693        0.410072    0.377483            278.0   \n",
       "5          16           0.000000        0.000000    0.000000            278.0   \n",
       "6          16           0.926966        0.593525    0.723684            278.0   \n",
       "7          16           0.852761        0.500000    0.630385            278.0   \n",
       "\n",
       "   Counting & Probability_precision  Counting & Probability_recall  \\\n",
       "0                          0.778689                       0.805085   \n",
       "1                          0.805085                       0.805085   \n",
       "2                          0.785714                       0.745763   \n",
       "3                          0.777778                       0.771186   \n",
       "4                          0.630137                       0.389831   \n",
       "5                          0.000000                       0.000000   \n",
       "6                          0.860465                       0.627119   \n",
       "7                          0.743363                       0.711864   \n",
       "\n",
       "   Counting & Probability_f1  Counting & Probability_support  \\\n",
       "0                   0.791667                           118.0   \n",
       "1                   0.805085                           118.0   \n",
       "2                   0.765217                           118.0   \n",
       "3                   0.774468                           118.0   \n",
       "4                   0.481675                           118.0   \n",
       "5                   0.000000                           118.0   \n",
       "6                   0.725490                           118.0   \n",
       "7                   0.727273                           118.0   \n",
       "\n",
       "   Geometry_precision  Geometry_recall  Geometry_f1  Geometry_support  \\\n",
       "0            0.880734         0.721805     0.793388             133.0   \n",
       "1            0.912088         0.624060     0.741071             133.0   \n",
       "2            0.822917         0.593985     0.689956             133.0   \n",
       "3            0.783019         0.624060     0.694561             133.0   \n",
       "4            0.411765         0.473684     0.440559             133.0   \n",
       "5            0.250000         0.015038     0.028369             133.0   \n",
       "6            0.796117         0.616541     0.694915             133.0   \n",
       "7            0.660550         0.541353     0.595041             133.0   \n",
       "\n",
       "   Intermediate Algebra_precision  Intermediate Algebra_recall  \\\n",
       "0                        0.915612                     0.927350   \n",
       "1                        0.931330                     0.927350   \n",
       "2                        0.955224                     0.820513   \n",
       "3                        0.984043                     0.790598   \n",
       "4                        0.293375                     0.397436   \n",
       "5                        0.000000                     0.000000   \n",
       "6                        0.769784                     0.914530   \n",
       "7                        0.651652                     0.927350   \n",
       "\n",
       "   Intermediate Algebra_f1  Intermediate Algebra_support  \\\n",
       "0                 0.921444                         234.0   \n",
       "1                 0.929336                         234.0   \n",
       "2                 0.882759                         234.0   \n",
       "3                 0.876777                         234.0   \n",
       "4                 0.337568                         234.0   \n",
       "5                 0.000000                         234.0   \n",
       "6                 0.835938                         234.0   \n",
       "7                 0.765432                         234.0   \n",
       "\n",
       "   Number Theory_precision  Number Theory_recall  Number Theory_f1  \\\n",
       "0                 0.852459              0.702703          0.770370   \n",
       "1                 0.765432              0.837838          0.800000   \n",
       "2                 0.756906              0.925676          0.832827   \n",
       "3                 0.784884              0.912162          0.843750   \n",
       "4                 0.425926              0.777027          0.550239   \n",
       "5                 0.000000              0.000000          0.000000   \n",
       "6                 0.802721              0.797297          0.800000   \n",
       "7                 0.811966              0.641892          0.716981   \n",
       "\n",
       "   Number Theory_support  Prealgebra_precision  Prealgebra_recall  \\\n",
       "0                  148.0              0.608059           0.761468   \n",
       "1                  148.0              0.620301           0.756881   \n",
       "2                  148.0              0.783582           0.481651   \n",
       "3                  148.0              0.793388           0.440367   \n",
       "4                  148.0              0.552632           0.096330   \n",
       "5                  148.0              0.192982           0.100917   \n",
       "6                  148.0              0.601399           0.788991   \n",
       "7                  148.0              0.573529           0.536697   \n",
       "\n",
       "   Prealgebra_f1  Prealgebra_support  Precalculus_precision  \\\n",
       "0       0.676171               218.0               0.950820   \n",
       "1       0.681818               218.0               0.966102   \n",
       "2       0.596591               218.0               0.941176   \n",
       "3       0.566372               218.0               0.858209   \n",
       "4       0.164062               218.0               1.000000   \n",
       "5       0.132530               218.0               0.562500   \n",
       "6       0.682540               218.0               0.822695   \n",
       "7       0.554502               218.0               0.905882   \n",
       "\n",
       "   Precalculus_recall  Precalculus_f1  Precalculus_support  \n",
       "0            0.958678        0.954733                121.0  \n",
       "1            0.942149        0.953975                121.0  \n",
       "2            0.925620        0.933333                121.0  \n",
       "3            0.950413        0.901961                121.0  \n",
       "4            0.008264        0.016393                121.0  \n",
       "5            0.148760        0.235294                121.0  \n",
       "6            0.958678        0.885496                121.0  \n",
       "7            0.636364        0.747573                121.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load results (assuming notebook is in notebooks/ and csv is in root)\n",
    "try:\n",
    "    df = pd.read_csv('../fine_tunings/unsloth/experiments.csv')\n",
    "except FileNotFoundError:\n",
    "    # Fallback if running from root\n",
    "    df = pd.read_csv('experiments.csv')\n",
    "\n",
    "print(f\"Total experiments: {len(df)}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fine_tunings/finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.822848</td>\n",
       "      <td>0.842657</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>20.863322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/finetune_mistral_7b/checkpoints/c...</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8072</td>\n",
       "      <td>0.819253</td>\n",
       "      <td>0.840247</td>\n",
       "      <td>0.8072</td>\n",
       "      <td>21.231163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine_tunings/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>20.748607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine_tunings/finetune_llama3_8b/checkpoints/ch...</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7640</td>\n",
       "      <td>0.783393</td>\n",
       "      <td>0.829615</td>\n",
       "      <td>0.7640</td>\n",
       "      <td>20.687743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fine_tunings/finetune_qwen2_7b/</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.759331</td>\n",
       "      <td>0.795759</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>15.381561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>0.479597</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>21.537799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mistralai/Mistral-7B-v0.3</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>0.114706</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>21.940266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model_path  \\\n",
       "1                   fine_tunings/finetune_mistral_7b   \n",
       "0  fine_tunings/finetune_mistral_7b/checkpoints/c...   \n",
       "2                    fine_tunings/finetune_llama3_8b   \n",
       "3  fine_tunings/finetune_llama3_8b/checkpoints/ch...   \n",
       "6                    fine_tunings/finetune_qwen2_7b/   \n",
       "4                         meta-llama/Meta-Llama-3-8B   \n",
       "5                          mistralai/Mistral-7B-v0.3   \n",
       "\n",
       "                     dataset  test_size  accuracy  f1_weighted  \\\n",
       "1  qwedsacf/competition_math       1250    0.8136     0.822848   \n",
       "0  qwedsacf/competition_math       1250    0.8072     0.819253   \n",
       "2  qwedsacf/competition_math       1250    0.7760     0.789775   \n",
       "3  qwedsacf/competition_math       1250    0.7640     0.783393   \n",
       "6  qwedsacf/competition_math       1250    0.7528     0.759331   \n",
       "4  qwedsacf/competition_math       1250    0.3624     0.334838   \n",
       "5  qwedsacf/competition_math       1250    0.0336     0.048908   \n",
       "\n",
       "   precision_weighted  recall_weighted  tokens_per_sec  \n",
       "1            0.842657           0.8136       20.863322  \n",
       "0            0.840247           0.8072       21.231163  \n",
       "2            0.827533           0.7760       20.748607  \n",
       "3            0.829615           0.7640       20.687743  \n",
       "6            0.795759           0.7528       15.381561  \n",
       "4            0.479597           0.3624       21.537799  \n",
       "5            0.114706           0.0336       21.940266  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key metrics comparison\n",
    "summary_cols = ['model_path', 'dataset', 'test_size', 'accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted', 'tokens_per_sec']\n",
    "available_cols = [c for c in summary_cols if c in df.columns]\n",
    "df[available_cols].sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 per-class F1 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>Algebra</th>\n",
       "      <th>Counting &amp; Probability</th>\n",
       "      <th>Geometry</th>\n",
       "      <th>Intermediate Algebra</th>\n",
       "      <th>Number Theory</th>\n",
       "      <th>Prealgebra</th>\n",
       "      <th>Precalculus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/finetune_mistral_7b/checkpoints/c...</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.836576</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>0.921444</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>0.676171</td>\n",
       "      <td>0.954733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fine_tunings/finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.845560</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.953975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine_tunings/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine_tunings/finetune_llama3_8b/checkpoints/ch...</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.837521</td>\n",
       "      <td>0.774468</td>\n",
       "      <td>0.694561</td>\n",
       "      <td>0.876777</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.377483</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>0.440559</td>\n",
       "      <td>0.337568</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.016393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mistralai/Mistral-7B-v0.3</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fine_tunings/finetune_qwen2_7b/</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.885496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model_path  \\\n",
       "0  fine_tunings/finetune_mistral_7b/checkpoints/c...   \n",
       "1                   fine_tunings/finetune_mistral_7b   \n",
       "2                    fine_tunings/finetune_llama3_8b   \n",
       "3  fine_tunings/finetune_llama3_8b/checkpoints/ch...   \n",
       "4                         meta-llama/Meta-Llama-3-8B   \n",
       "5                          mistralai/Mistral-7B-v0.3   \n",
       "6                    fine_tunings/finetune_qwen2_7b/   \n",
       "\n",
       "                     dataset  test_size   Algebra  Counting & Probability  \\\n",
       "0  qwedsacf/competition_math       1250  0.836576                0.791667   \n",
       "1  qwedsacf/competition_math       1250  0.845560                0.805085   \n",
       "2  qwedsacf/competition_math       1250  0.835772                0.765217   \n",
       "3  qwedsacf/competition_math       1250  0.837521                0.774468   \n",
       "4  qwedsacf/competition_math       1250  0.377483                0.481675   \n",
       "5  qwedsacf/competition_math       1250  0.000000                0.000000   \n",
       "6  qwedsacf/competition_math       1250  0.723684                0.725490   \n",
       "\n",
       "   Geometry  Intermediate Algebra  Number Theory  Prealgebra  Precalculus  \n",
       "0  0.793388              0.921444       0.770370    0.676171     0.954733  \n",
       "1  0.741071              0.929336       0.800000    0.681818     0.953975  \n",
       "2  0.689956              0.882759       0.832827    0.596591     0.933333  \n",
       "3  0.694561              0.876777       0.843750    0.566372     0.901961  \n",
       "4  0.440559              0.337568       0.550239    0.164062     0.016393  \n",
       "5  0.028369              0.000000       0.000000    0.132530     0.235294  \n",
       "6  0.694915              0.835938       0.800000    0.682540     0.885496  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract per-class F1 columns\n",
    "f1_cols = [c for c in df.columns if c.endswith('_f1') and c != 'f1_weighted']\n",
    "print(f\"Found {len(f1_cols)} per-class F1 columns.\")\n",
    "if f1_cols:\n",
    "    display_df = df[['model_path', 'dataset', 'test_size',] + f1_cols].copy()\n",
    "    display_df.columns = ['model_path', 'dataset', 'test_size',] + [c.replace('_f1', '') for c in f1_cols]\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "num_epochs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "batch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lora_rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lora_alpha",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1d441c16-8e22-46c6-a4ce-064b5ce87a55",
       "rows": [
        [
         "0",
         "fine_tunings/finetune_mistral_7b/checkpoints/checkpoint-3750",
         "mistral:7b",
         "3",
         "2",
         "0.0002",
         "16",
         "16"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>model_name</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>lora_rank</th>\n",
       "      <th>lora_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/finetune_mistral_7b/checkpoints/c...</td>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model_path  model_name  num_epochs  \\\n",
       "0  fine_tunings/finetune_mistral_7b/checkpoints/c...  mistral:7b           3   \n",
       "\n",
       "   batch_size  learning_rate  lora_rank  lora_alpha  \n",
       "0           2         0.0002         16          16  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config columns\n",
    "config_cols = ['model_path', 'model_name', 'num_epochs', 'batch_size', 'learning_rate', 'lora_rank', 'lora_alpha']\n",
    "available_config = [c for c in config_cols if c in df.columns]\n",
    "df[available_config]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advancednlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
