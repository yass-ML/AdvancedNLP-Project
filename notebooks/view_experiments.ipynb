{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results Viewer\n",
    "\n",
    "This notebook displays the results from fine-tuning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model_path</th>\n",
       "      <th>model_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>lora_rank</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>Algebra_precision</th>\n",
       "      <th>Algebra_recall</th>\n",
       "      <th>Algebra_f1</th>\n",
       "      <th>Algebra_support</th>\n",
       "      <th>Counting &amp; Probability_precision</th>\n",
       "      <th>Counting &amp; Probability_recall</th>\n",
       "      <th>Counting &amp; Probability_f1</th>\n",
       "      <th>Counting &amp; Probability_support</th>\n",
       "      <th>Geometry_precision</th>\n",
       "      <th>Geometry_recall</th>\n",
       "      <th>Geometry_f1</th>\n",
       "      <th>Geometry_support</th>\n",
       "      <th>Intermediate Algebra_precision</th>\n",
       "      <th>Intermediate Algebra_recall</th>\n",
       "      <th>Intermediate Algebra_f1</th>\n",
       "      <th>Intermediate Algebra_support</th>\n",
       "      <th>Number Theory_precision</th>\n",
       "      <th>Number Theory_recall</th>\n",
       "      <th>Number Theory_f1</th>\n",
       "      <th>Number Theory_support</th>\n",
       "      <th>Prealgebra_precision</th>\n",
       "      <th>Prealgebra_recall</th>\n",
       "      <th>Prealgebra_f1</th>\n",
       "      <th>Prealgebra_support</th>\n",
       "      <th>Precalculus_precision</th>\n",
       "      <th>Precalculus_recall</th>\n",
       "      <th>Precalculus_f1</th>\n",
       "      <th>Precalculus_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-03 19:02:25</td>\n",
       "      <td>fine_tunings/llama3:8b_defaultLoRA_2epochs/</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>llama3:8b_defaultLoRA_2epochs</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.854538</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8008</td>\n",
       "      <td>20.411347</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.6</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                                   model_path  \\\n",
       "0  2026-01-03 19:02:25  fine_tunings/llama3:8b_defaultLoRA_2epochs/   \n",
       "\n",
       "  model_name                       run_name                    dataset  \\\n",
       "0  llama3:8b  llama3:8b_defaultLoRA_2epochs  qwedsacf/competition_math   \n",
       "\n",
       "   accuracy  precision_weighted  recall_weighted  f1_weighted  tokens_per_sec  \\\n",
       "0       0.8            0.854538              0.8       0.8008       20.411347   \n",
       "\n",
       "   num_epochs  batch_size  learning_rate  lora_rank  lora_alpha  \\\n",
       "0           2           2         0.0002         16          16   \n",
       "\n",
       "   Algebra_precision  Algebra_recall  Algebra_f1  Algebra_support  \\\n",
       "0           0.793103            0.92    0.851852             25.0   \n",
       "\n",
       "   Counting & Probability_precision  Counting & Probability_recall  \\\n",
       "0                          0.666667                       0.666667   \n",
       "\n",
       "   Counting & Probability_f1  Counting & Probability_support  \\\n",
       "0                   0.666667                             6.0   \n",
       "\n",
       "   Geometry_precision  Geometry_recall  Geometry_f1  Geometry_support  \\\n",
       "0                 0.7              0.7          0.7              10.0   \n",
       "\n",
       "   Intermediate Algebra_precision  Intermediate Algebra_recall  \\\n",
       "0                        0.909091                     0.952381   \n",
       "\n",
       "   Intermediate Algebra_f1  Intermediate Algebra_support  \\\n",
       "0                 0.930233                          21.0   \n",
       "\n",
       "   Number Theory_precision  Number Theory_recall  Number Theory_f1  \\\n",
       "0                 0.777778                   1.0             0.875   \n",
       "\n",
       "   Number Theory_support  Prealgebra_precision  Prealgebra_recall  \\\n",
       "0                    7.0                   1.0           0.428571   \n",
       "\n",
       "   Prealgebra_f1  Prealgebra_support  Precalculus_precision  \\\n",
       "0            0.6                21.0               0.909091   \n",
       "\n",
       "   Precalculus_recall  Precalculus_f1  Precalculus_support  \n",
       "0                 1.0        0.952381                 10.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load results (assuming notebook is in notebooks/ and csv is in root)\n",
    "try:\n",
    "    df = pd.read_csv('../experiments.csv')\n",
    "except FileNotFoundError:\n",
    "    # Fallback if running from root\n",
    "    df = pd.read_csv('experiments.csv')\n",
    "\n",
    "print(f\"Total experiments: {len(df)}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3:8b_defaultLoRA_2epochs</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8008</td>\n",
       "      <td>0.854538</td>\n",
       "      <td>0.8</td>\n",
       "      <td>20.411347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        run_name  accuracy  f1_weighted  precision_weighted  \\\n",
       "0  llama3:8b_defaultLoRA_2epochs       0.8       0.8008            0.854538   \n",
       "\n",
       "   recall_weighted  tokens_per_sec  \n",
       "0              0.8       20.411347  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key metrics comparison\n",
    "summary_cols = ['run_name', 'accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted', 'tokens_per_sec']\n",
    "available_cols = [c for c in summary_cols if c in df.columns]\n",
    "df[available_cols].sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 per-class F1 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>Algebra</th>\n",
       "      <th>Counting &amp; Probability</th>\n",
       "      <th>Geometry</th>\n",
       "      <th>Intermediate Algebra</th>\n",
       "      <th>Number Theory</th>\n",
       "      <th>Prealgebra</th>\n",
       "      <th>Precalculus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3:8b_defaultLoRA_2epochs</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        run_name   Algebra  Counting & Probability  Geometry  \\\n",
       "0  llama3:8b_defaultLoRA_2epochs  0.851852                0.666667       0.7   \n",
       "\n",
       "   Intermediate Algebra  Number Theory  Prealgebra  Precalculus  \n",
       "0              0.930233          0.875         0.6     0.952381  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract per-class F1 columns\n",
    "f1_cols = [c for c in df.columns if c.endswith('_f1') and c != 'f1_weighted']\n",
    "print(f\"Found {len(f1_cols)} per-class F1 columns.\")\n",
    "if f1_cols:\n",
    "    display_df = df[['run_name'] + f1_cols].copy()\n",
    "    display_df.columns = ['run_name'] + [c.replace('_f1', '') for c in f1_cols]\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>lora_rank</th>\n",
       "      <th>lora_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3:8b_defaultLoRA_2epochs</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        run_name model_name  num_epochs  batch_size  \\\n",
       "0  llama3:8b_defaultLoRA_2epochs  llama3:8b           2           2   \n",
       "\n",
       "   learning_rate  lora_rank  lora_alpha  \n",
       "0         0.0002         16          16  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config columns\n",
    "config_cols = ['run_name', 'model_name', 'num_epochs', 'batch_size', 'learning_rate', 'lora_rank', 'lora_alpha']\n",
    "available_config = [c for c in config_cols if c in df.columns]\n",
    "df[available_config]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advancednlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
