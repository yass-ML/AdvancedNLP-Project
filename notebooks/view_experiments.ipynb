{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results Viewer\n",
    "\n",
    "This notebook displays the results from fine-tuning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments: 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model_path</th>\n",
       "      <th>model_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>lora_rank</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>Algebra_precision</th>\n",
       "      <th>Algebra_recall</th>\n",
       "      <th>Algebra_f1</th>\n",
       "      <th>Algebra_support</th>\n",
       "      <th>Counting &amp; Probability_precision</th>\n",
       "      <th>Counting &amp; Probability_recall</th>\n",
       "      <th>Counting &amp; Probability_f1</th>\n",
       "      <th>Counting &amp; Probability_support</th>\n",
       "      <th>Geometry_precision</th>\n",
       "      <th>Geometry_recall</th>\n",
       "      <th>Geometry_f1</th>\n",
       "      <th>Geometry_support</th>\n",
       "      <th>Intermediate Algebra_precision</th>\n",
       "      <th>Intermediate Algebra_recall</th>\n",
       "      <th>Intermediate Algebra_f1</th>\n",
       "      <th>Intermediate Algebra_support</th>\n",
       "      <th>Number Theory_precision</th>\n",
       "      <th>Number Theory_recall</th>\n",
       "      <th>Number Theory_f1</th>\n",
       "      <th>Number Theory_support</th>\n",
       "      <th>Prealgebra_precision</th>\n",
       "      <th>Prealgebra_recall</th>\n",
       "      <th>Prealgebra_f1</th>\n",
       "      <th>Prealgebra_support</th>\n",
       "      <th>Precalculus_precision</th>\n",
       "      <th>Precalculus_recall</th>\n",
       "      <th>Precalculus_f1</th>\n",
       "      <th>Precalculus_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-04 12:50:25</td>\n",
       "      <td>fine_tunings/finetune_mistral_7b/checkpoints/c...</td>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8072</td>\n",
       "      <td>0.840247</td>\n",
       "      <td>0.8072</td>\n",
       "      <td>0.819253</td>\n",
       "      <td>21.231163</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.911017</td>\n",
       "      <td>0.773381</td>\n",
       "      <td>0.836576</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.915612</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.921444</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.608059</td>\n",
       "      <td>0.761468</td>\n",
       "      <td>0.676171</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.954733</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-04 13:10:49</td>\n",
       "      <td>fine_tunings/finetune_mistral_7b</td>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.842657</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.822848</td>\n",
       "      <td>20.863322</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.787770</td>\n",
       "      <td>0.845560</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.931330</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.620301</td>\n",
       "      <td>0.756881</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.942149</td>\n",
       "      <td>0.953975</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-04 13:37:46</td>\n",
       "      <td>fine_tunings/finetune_llama3_8b</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>20.748607</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.762611</td>\n",
       "      <td>0.924460</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.593985</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.955224</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.756906</td>\n",
       "      <td>0.925676</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.481651</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-04 13:48:44</td>\n",
       "      <td>fine_tunings/finetune_llama3_8b/checkpoints/ch...</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7640</td>\n",
       "      <td>0.829615</td>\n",
       "      <td>0.7640</td>\n",
       "      <td>0.783393</td>\n",
       "      <td>20.687743</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.783699</td>\n",
       "      <td>0.899281</td>\n",
       "      <td>0.837521</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.771186</td>\n",
       "      <td>0.774468</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.783019</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.694561</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.876777</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.784884</td>\n",
       "      <td>0.912162</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>0.440367</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.950413</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-04 14:04:38</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.479597</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>21.537799</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.349693</td>\n",
       "      <td>0.410072</td>\n",
       "      <td>0.377483</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.630137</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.440559</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.293375</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.337568</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.096330</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-01-04 14:14:33</td>\n",
       "      <td>mistralai/Mistral-7B-v0.3</td>\n",
       "      <td>mistralai/Mistral-7B-v0.3</td>\n",
       "      <td>base_model_eval</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.114706</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>21.940266</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.100917</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-01-05 15:25:43</td>\n",
       "      <td>fine_tunings/finetune_qwen2_7b/</td>\n",
       "      <td>qwen2:7b</td>\n",
       "      <td>finetune_qwen2_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.795759</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.759331</td>\n",
       "      <td>15.381561</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.616541</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.601399</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                                         model_path  \\\n",
       "0  2026-01-04 12:50:25  fine_tunings/finetune_mistral_7b/checkpoints/c...   \n",
       "1  2026-01-04 13:10:49                   fine_tunings/finetune_mistral_7b   \n",
       "2  2026-01-04 13:37:46                    fine_tunings/finetune_llama3_8b   \n",
       "3  2026-01-04 13:48:44  fine_tunings/finetune_llama3_8b/checkpoints/ch...   \n",
       "4  2026-01-04 14:04:38                         meta-llama/Meta-Llama-3-8B   \n",
       "5  2026-01-04 14:14:33                          mistralai/Mistral-7B-v0.3   \n",
       "6  2026-01-05 15:25:43                    fine_tunings/finetune_qwen2_7b/   \n",
       "\n",
       "                   model_name             run_name                    dataset  \\\n",
       "0                  mistral:7b  finetune_mistral_7b  qwedsacf/competition_math   \n",
       "1                  mistral:7b  finetune_mistral_7b  qwedsacf/competition_math   \n",
       "2                   llama3:8b   finetune_llama3_8b  qwedsacf/competition_math   \n",
       "3                   llama3:8b   finetune_llama3_8b  qwedsacf/competition_math   \n",
       "4  meta-llama/Meta-Llama-3-8B      base_model_eval  qwedsacf/competition_math   \n",
       "5   mistralai/Mistral-7B-v0.3      base_model_eval  qwedsacf/competition_math   \n",
       "6                    qwen2:7b    finetune_qwen2_7b  qwedsacf/competition_math   \n",
       "\n",
       "   test_size  accuracy  precision_weighted  recall_weighted  f1_weighted  \\\n",
       "0       1250    0.8072            0.840247           0.8072     0.819253   \n",
       "1       1250    0.8136            0.842657           0.8136     0.822848   \n",
       "2       1250    0.7760            0.827533           0.7760     0.789775   \n",
       "3       1250    0.7640            0.829615           0.7640     0.783393   \n",
       "4       1250    0.3624            0.479597           0.3624     0.334838   \n",
       "5       1250    0.0336            0.114706           0.0336     0.048908   \n",
       "6       1250    0.7528            0.795759           0.7528     0.759331   \n",
       "\n",
       "   tokens_per_sec  num_epochs  batch_size  learning_rate  lora_rank  \\\n",
       "0       21.231163           3           2         0.0002         16   \n",
       "1       20.863322           3           2         0.0002         16   \n",
       "2       20.748607           3           2         0.0002         16   \n",
       "3       20.687743           3           2         0.0002         16   \n",
       "4       21.537799           0           2         0.0002         16   \n",
       "5       21.940266           0           2         0.0002         16   \n",
       "6       15.381561           3           2         0.0002         16   \n",
       "\n",
       "   lora_alpha  Algebra_precision  Algebra_recall  Algebra_f1  Algebra_support  \\\n",
       "0          16           0.911017        0.773381    0.836576            278.0   \n",
       "1          16           0.912500        0.787770    0.845560            278.0   \n",
       "2          16           0.762611        0.924460    0.835772            278.0   \n",
       "3          16           0.783699        0.899281    0.837521            278.0   \n",
       "4          16           0.349693        0.410072    0.377483            278.0   \n",
       "5          16           0.000000        0.000000    0.000000            278.0   \n",
       "6          16           0.926966        0.593525    0.723684            278.0   \n",
       "\n",
       "   Counting & Probability_precision  Counting & Probability_recall  \\\n",
       "0                          0.778689                       0.805085   \n",
       "1                          0.805085                       0.805085   \n",
       "2                          0.785714                       0.745763   \n",
       "3                          0.777778                       0.771186   \n",
       "4                          0.630137                       0.389831   \n",
       "5                          0.000000                       0.000000   \n",
       "6                          0.860465                       0.627119   \n",
       "\n",
       "   Counting & Probability_f1  Counting & Probability_support  \\\n",
       "0                   0.791667                           118.0   \n",
       "1                   0.805085                           118.0   \n",
       "2                   0.765217                           118.0   \n",
       "3                   0.774468                           118.0   \n",
       "4                   0.481675                           118.0   \n",
       "5                   0.000000                           118.0   \n",
       "6                   0.725490                           118.0   \n",
       "\n",
       "   Geometry_precision  Geometry_recall  Geometry_f1  Geometry_support  \\\n",
       "0            0.880734         0.721805     0.793388             133.0   \n",
       "1            0.912088         0.624060     0.741071             133.0   \n",
       "2            0.822917         0.593985     0.689956             133.0   \n",
       "3            0.783019         0.624060     0.694561             133.0   \n",
       "4            0.411765         0.473684     0.440559             133.0   \n",
       "5            0.250000         0.015038     0.028369             133.0   \n",
       "6            0.796117         0.616541     0.694915             133.0   \n",
       "\n",
       "   Intermediate Algebra_precision  Intermediate Algebra_recall  \\\n",
       "0                        0.915612                     0.927350   \n",
       "1                        0.931330                     0.927350   \n",
       "2                        0.955224                     0.820513   \n",
       "3                        0.984043                     0.790598   \n",
       "4                        0.293375                     0.397436   \n",
       "5                        0.000000                     0.000000   \n",
       "6                        0.769784                     0.914530   \n",
       "\n",
       "   Intermediate Algebra_f1  Intermediate Algebra_support  \\\n",
       "0                 0.921444                         234.0   \n",
       "1                 0.929336                         234.0   \n",
       "2                 0.882759                         234.0   \n",
       "3                 0.876777                         234.0   \n",
       "4                 0.337568                         234.0   \n",
       "5                 0.000000                         234.0   \n",
       "6                 0.835938                         234.0   \n",
       "\n",
       "   Number Theory_precision  Number Theory_recall  Number Theory_f1  \\\n",
       "0                 0.852459              0.702703          0.770370   \n",
       "1                 0.765432              0.837838          0.800000   \n",
       "2                 0.756906              0.925676          0.832827   \n",
       "3                 0.784884              0.912162          0.843750   \n",
       "4                 0.425926              0.777027          0.550239   \n",
       "5                 0.000000              0.000000          0.000000   \n",
       "6                 0.802721              0.797297          0.800000   \n",
       "\n",
       "   Number Theory_support  Prealgebra_precision  Prealgebra_recall  \\\n",
       "0                  148.0              0.608059           0.761468   \n",
       "1                  148.0              0.620301           0.756881   \n",
       "2                  148.0              0.783582           0.481651   \n",
       "3                  148.0              0.793388           0.440367   \n",
       "4                  148.0              0.552632           0.096330   \n",
       "5                  148.0              0.192982           0.100917   \n",
       "6                  148.0              0.601399           0.788991   \n",
       "\n",
       "   Prealgebra_f1  Prealgebra_support  Precalculus_precision  \\\n",
       "0       0.676171               218.0               0.950820   \n",
       "1       0.681818               218.0               0.966102   \n",
       "2       0.596591               218.0               0.941176   \n",
       "3       0.566372               218.0               0.858209   \n",
       "4       0.164062               218.0               1.000000   \n",
       "5       0.132530               218.0               0.562500   \n",
       "6       0.682540               218.0               0.822695   \n",
       "\n",
       "   Precalculus_recall  Precalculus_f1  Precalculus_support  \n",
       "0            0.958678        0.954733                121.0  \n",
       "1            0.942149        0.953975                121.0  \n",
       "2            0.925620        0.933333                121.0  \n",
       "3            0.950413        0.901961                121.0  \n",
       "4            0.008264        0.016393                121.0  \n",
       "5            0.148760        0.235294                121.0  \n",
       "6            0.958678        0.885496                121.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load results (assuming notebook is in notebooks/ and csv is in root)\n",
    "try:\n",
    "    df = pd.read_csv('../fine_tunings/experiments.csv')\n",
    "except FileNotFoundError:\n",
    "    # Fallback if running from root\n",
    "    df = pd.read_csv('experiments.csv')\n",
    "\n",
    "print(f\"Total experiments: {len(df)}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fine_tunings/finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.822848</td>\n",
       "      <td>0.842657</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>20.863322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/finetune_mistral_7b/checkpoints/c...</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.8072</td>\n",
       "      <td>0.819253</td>\n",
       "      <td>0.840247</td>\n",
       "      <td>0.8072</td>\n",
       "      <td>21.231163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine_tunings/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.789775</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>20.748607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine_tunings/finetune_llama3_8b/checkpoints/ch...</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7640</td>\n",
       "      <td>0.783393</td>\n",
       "      <td>0.829615</td>\n",
       "      <td>0.7640</td>\n",
       "      <td>20.687743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fine_tunings/finetune_qwen2_7b/</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>0.759331</td>\n",
       "      <td>0.795759</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>15.381561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>0.334838</td>\n",
       "      <td>0.479597</td>\n",
       "      <td>0.3624</td>\n",
       "      <td>21.537799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mistralai/Mistral-7B-v0.3</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>0.114706</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>21.940266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model_path  \\\n",
       "1                   fine_tunings/finetune_mistral_7b   \n",
       "0  fine_tunings/finetune_mistral_7b/checkpoints/c...   \n",
       "2                    fine_tunings/finetune_llama3_8b   \n",
       "3  fine_tunings/finetune_llama3_8b/checkpoints/ch...   \n",
       "6                    fine_tunings/finetune_qwen2_7b/   \n",
       "4                         meta-llama/Meta-Llama-3-8B   \n",
       "5                          mistralai/Mistral-7B-v0.3   \n",
       "\n",
       "                     dataset  test_size  accuracy  f1_weighted  \\\n",
       "1  qwedsacf/competition_math       1250    0.8136     0.822848   \n",
       "0  qwedsacf/competition_math       1250    0.8072     0.819253   \n",
       "2  qwedsacf/competition_math       1250    0.7760     0.789775   \n",
       "3  qwedsacf/competition_math       1250    0.7640     0.783393   \n",
       "6  qwedsacf/competition_math       1250    0.7528     0.759331   \n",
       "4  qwedsacf/competition_math       1250    0.3624     0.334838   \n",
       "5  qwedsacf/competition_math       1250    0.0336     0.048908   \n",
       "\n",
       "   precision_weighted  recall_weighted  tokens_per_sec  \n",
       "1            0.842657           0.8136       20.863322  \n",
       "0            0.840247           0.8072       21.231163  \n",
       "2            0.827533           0.7760       20.748607  \n",
       "3            0.829615           0.7640       20.687743  \n",
       "6            0.795759           0.7528       15.381561  \n",
       "4            0.479597           0.3624       21.537799  \n",
       "5            0.114706           0.0336       21.940266  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key metrics comparison\n",
    "summary_cols = ['model_path', 'dataset', 'test_size', 'accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted', 'tokens_per_sec']\n",
    "available_cols = [c for c in summary_cols if c in df.columns]\n",
    "df[available_cols].sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 per-class F1 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_size</th>\n",
       "      <th>Algebra</th>\n",
       "      <th>Counting &amp; Probability</th>\n",
       "      <th>Geometry</th>\n",
       "      <th>Intermediate Algebra</th>\n",
       "      <th>Number Theory</th>\n",
       "      <th>Prealgebra</th>\n",
       "      <th>Precalculus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/finetune_mistral_7b/checkpoints/c...</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.836576</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>0.921444</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>0.676171</td>\n",
       "      <td>0.954733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fine_tunings/finetune_mistral_7b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.845560</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.953975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine_tunings/finetune_llama3_8b</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.835772</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.832827</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine_tunings/finetune_llama3_8b/checkpoints/ch...</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.837521</td>\n",
       "      <td>0.774468</td>\n",
       "      <td>0.694561</td>\n",
       "      <td>0.876777</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.377483</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>0.440559</td>\n",
       "      <td>0.337568</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.016393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mistralai/Mistral-7B-v0.3</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fine_tunings/finetune_qwen2_7b/</td>\n",
       "      <td>qwedsacf/competition_math</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.885496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model_path  \\\n",
       "0  fine_tunings/finetune_mistral_7b/checkpoints/c...   \n",
       "1                   fine_tunings/finetune_mistral_7b   \n",
       "2                    fine_tunings/finetune_llama3_8b   \n",
       "3  fine_tunings/finetune_llama3_8b/checkpoints/ch...   \n",
       "4                         meta-llama/Meta-Llama-3-8B   \n",
       "5                          mistralai/Mistral-7B-v0.3   \n",
       "6                    fine_tunings/finetune_qwen2_7b/   \n",
       "\n",
       "                     dataset  test_size   Algebra  Counting & Probability  \\\n",
       "0  qwedsacf/competition_math       1250  0.836576                0.791667   \n",
       "1  qwedsacf/competition_math       1250  0.845560                0.805085   \n",
       "2  qwedsacf/competition_math       1250  0.835772                0.765217   \n",
       "3  qwedsacf/competition_math       1250  0.837521                0.774468   \n",
       "4  qwedsacf/competition_math       1250  0.377483                0.481675   \n",
       "5  qwedsacf/competition_math       1250  0.000000                0.000000   \n",
       "6  qwedsacf/competition_math       1250  0.723684                0.725490   \n",
       "\n",
       "   Geometry  Intermediate Algebra  Number Theory  Prealgebra  Precalculus  \n",
       "0  0.793388              0.921444       0.770370    0.676171     0.954733  \n",
       "1  0.741071              0.929336       0.800000    0.681818     0.953975  \n",
       "2  0.689956              0.882759       0.832827    0.596591     0.933333  \n",
       "3  0.694561              0.876777       0.843750    0.566372     0.901961  \n",
       "4  0.440559              0.337568       0.550239    0.164062     0.016393  \n",
       "5  0.028369              0.000000       0.000000    0.132530     0.235294  \n",
       "6  0.694915              0.835938       0.800000    0.682540     0.885496  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract per-class F1 columns\n",
    "f1_cols = [c for c in df.columns if c.endswith('_f1') and c != 'f1_weighted']\n",
    "print(f\"Found {len(f1_cols)} per-class F1 columns.\")\n",
    "if f1_cols:\n",
    "    display_df = df[['model_path', 'dataset', 'test_size',] + f1_cols].copy()\n",
    "    display_df.columns = ['model_path', 'dataset', 'test_size',] + [c.replace('_f1', '') for c in f1_cols]\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "num_epochs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "batch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lora_rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lora_alpha",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1d441c16-8e22-46c6-a4ce-064b5ce87a55",
       "rows": [
        [
         "0",
         "fine_tunings/finetune_mistral_7b/checkpoints/checkpoint-3750",
         "mistral:7b",
         "3",
         "2",
         "0.0002",
         "16",
         "16"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "      <th>model_name</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>lora_rank</th>\n",
       "      <th>lora_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fine_tunings/finetune_mistral_7b/checkpoints/c...</td>\n",
       "      <td>mistral:7b</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model_path  model_name  num_epochs  \\\n",
       "0  fine_tunings/finetune_mistral_7b/checkpoints/c...  mistral:7b           3   \n",
       "\n",
       "   batch_size  learning_rate  lora_rank  lora_alpha  \n",
       "0           2         0.0002         16          16  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config columns\n",
    "config_cols = ['model_path', 'model_name', 'num_epochs', 'batch_size', 'learning_rate', 'lora_rank', 'lora_alpha']\n",
    "available_config = [c for c in config_cols if c in df.columns]\n",
    "df[available_config]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
